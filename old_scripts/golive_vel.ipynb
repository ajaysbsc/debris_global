{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot_array(dem, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None, close_fig=True):\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    #Gray background\n",
    "    ax.set_facecolor('0.5')\n",
    "    #Force aspect ratio to match images\n",
    "    ax.set(aspect='equal')\n",
    "    #Turn off axes labels/ticks\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if titles is not None:\n",
    "        ax.set_title(titles[0])\n",
    "    #Plot background shaded relief map\n",
    "    if overlay is not None:\n",
    "        alpha = 0.7\n",
    "        ax.imshow(overlay, cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [ax.imshow(dem, clim=clim, cmap=cmap, alpha=alpha)]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "    if close_fig:\n",
    "        plt.close(fig)\n",
    "\n",
    "def nearest_nonzero_idx(a,x,y):\n",
    "    r,c = np.nonzero(a)\n",
    "    min_idx = ((r - x)**2 + (c - y)**2).argmin()\n",
    "    return r[min_idx], c[min_idx]\n",
    "\n",
    "\n",
    "def maskedarray_gt(data, value, set_value=None):\n",
    "    \"\"\" Greater than operation on masked array to avoid warning errors \"\"\"\n",
    "    if set_value is None:\n",
    "        set_value = value\n",
    "    data = np.nan_to_num(data,0)\n",
    "    data[data > value] = set_value\n",
    "    return data\n",
    "\n",
    "\n",
    "def maskedarray_lt(data, value, set_value=None):\n",
    "    \"\"\" Less than operation on masked array to avoid warning errors \"\"\"\n",
    "    if set_value is None:\n",
    "        set_value = value\n",
    "    data = np.nan_to_num(data,0)\n",
    "    data[data < value] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "def emergence_pixels(gf, vel_x_raw, vel_y_raw, icethickness_raw, xres, yres, \n",
    "                     vel_min=0, max_velocity=600, vel_depth_avg_factor=0.8, option_border=1,\n",
    "                     positive_is_east=True, positive_is_north=True, constant_icethickness=False, debug=True):\n",
    "    \"\"\" Compute the emergence velocity using an ice flux approach\n",
    "    \"\"\"\n",
    "    # Glacier mask\n",
    "    glac_mask = np.zeros(vel_x_raw.shape) + 1\n",
    "    glac_mask[gf.z1.mask] = 0\n",
    "    \n",
    "    # Modify vel_y by multiplying velocity by -1 such that matrix operations agree with flow direction\n",
    "    #    Specifically, a negative y velocity means the pixel is flowing south.\n",
    "    #    However, if you were to subtract that value from the rows, it would head north in the matrix.\n",
    "    #    This is due to the fact that the number of rows start at 0 at the top.\n",
    "    #    Therefore, multipylying by -1 aligns the matrix operations with the flow direction\n",
    "    if positive_is_north:\n",
    "        vel_y = -1*vel_y_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_y = vel_y_raw * vel_depth_avg_factor\n",
    "    if positive_is_east:\n",
    "        vel_x = vel_x_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_x = -1*vel_x_raw * vel_depth_avg_factor\n",
    "    vel_total = (vel_y**2 + vel_x**2)**0.5\n",
    "    # Ice thickness\n",
    "    icethickness = icethickness_raw.copy()\n",
    "    if constant_icethickness:\n",
    "        icethickness[:,:] = 1\n",
    "        icethickness = icethickness * glac_mask\n",
    "#     print('mean ice thickness:', np.round(icethickness.mean(),0), 'm')\n",
    "    # Compute the initial volume\n",
    "    volume_initial = icethickness * (xres * yres)\n",
    "    pix_maxres = xres\n",
    "    if yres > pix_maxres:\n",
    "        pix_maxres = yres\n",
    "    # Quality control options:\n",
    "    # Apply a border based on the max specified velocity to prevent errors associated with pixels going out of bounds\n",
    "    if option_border == 1:\n",
    "        border = int(max_velocity / pix_maxres) + 1\n",
    "        for r in range(vel_x.shape[0]):\n",
    "            for c in range(vel_x.shape[1]):\n",
    "                if (r < border) | (r >= vel_x.shape[0] - border) | (c < border) | (c >= vel_x.shape[1] - border):\n",
    "                    vel_x[r,c] = 0\n",
    "                    vel_y[r,c] = 0\n",
    "    # Minimum/maximum velocity bounds\n",
    "    vel_x[vel_total < vel_min] = 0\n",
    "    vel_y[vel_total < vel_min] = 0\n",
    "    vel_x[vel_total > max_velocity] = 0\n",
    "    vel_y[vel_total > max_velocity] = 0\n",
    "#     # Remove clusters of high velocity on stagnant portions of glaciers due to feature tracking of ice cliffs and ponds\n",
    "#     if option_stagnantbands == 1:\n",
    "#         vel_x[bands <= stagnant_band] = 0\n",
    "#         vel_y[bands <= stagnant_band] = 0        \n",
    "    # Compute displacement in units of pixels\n",
    "    vel_x_pix = vel_x / xres\n",
    "    vel_y_pix = vel_y / yres\n",
    "    # Compute the displacement and fraction of pixels moved for all columns (x-axis)\n",
    "    # col_x1 is the number of columns to the closest pixel receiving ice [ex. 2.6 returns 2, -2.6 returns -2]\n",
    "    #    int() automatically rounds towards zero\n",
    "    col_x1 = vel_x_pix.astype(int)\n",
    "    # col_x2 is the number of columns to the further pixel receiving ice [ex. 2.6 returns 3, -2.6 returns -3]\n",
    "    #    np.sign() returns a value of 1 or -1, so it's adding 1 pixel away from zero\n",
    "    col_x2 = (vel_x_pix + np.sign(vel_x_pix)).astype(int)\n",
    "    # rem_x2 is the fraction of the pixel that remains in the further pixel (col_x2) [ex. 2.6 returns 0.6, -2.6 returns 0.6]\n",
    "    #    np.sign() returns a value of 1 or -1, so multiplying by that ensures you have a positive value\n",
    "    #    then when you take the remainder using \"% 1\", you obtain the desired fraction\n",
    "    rem_x2 = np.multiply(np.sign(vel_x_pix), vel_x_pix) % 1\n",
    "    # rem_x1 is the fraction of the pixel that remains in the closer pixel (col_x1) [ex. 2.6 returns 0.4, -2.6 returns 0.4]\n",
    "    rem_x1 = 1 - rem_x2\n",
    "    # Repeat the displacement and fraction computations for all rows (y-axis)\n",
    "    row_y1 = vel_y_pix.astype(int)\n",
    "    row_y2 = (vel_y_pix + np.sign(vel_y_pix)).astype(int)\n",
    "    rem_y2 = np.multiply(np.sign(vel_y_pix), vel_y_pix) % 1\n",
    "    rem_y1 = 1 - rem_y2\n",
    "          \n",
    "    # Compute the mass flux for each pixel\n",
    "    volume_final = np.zeros(volume_initial.shape)\n",
    "    for r in range(vel_x.shape[0]):\n",
    "        for c in range(vel_x.shape[1]):\n",
    "            volume_final[r+row_y1[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x1[r,c]] + rem_y1[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x1[r,c]] + rem_y2[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y1[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x2[r,c]] + rem_y1[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x2[r,c]] + rem_y2[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "         \n",
    "    # Redistribute off-glacier volume back onto the nearest pixel on the glacier\n",
    "    offglac_row, offglac_col = np.where((glac_mask == 0) & (volume_final > 0))\n",
    "    for nidx in range(0,len(offglac_row)):\n",
    "        nrow = offglac_row[nidx]\n",
    "        ncol = offglac_col[nidx]\n",
    "        ridx, cidx = nearest_nonzero_idx(glac_mask, nrow, ncol)\n",
    "        # Add off-glacier volume back onto nearest pixel on glacier\n",
    "        volume_final[ridx,cidx] += volume_final[nrow,ncol]\n",
    "        volume_final[nrow,ncol] = 0\n",
    "            \n",
    "    # Check that mass is conserved (threshold = 0.1 m x pixel_size**2)\n",
    "    if debug:\n",
    "        print('Mass is conserved?', np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() < 0.01)\n",
    "        print(np.round(np.absolute(volume_final.sum() - volume_initial.sum()),1), \n",
    "              np.round(np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() * 100,2), '%')\n",
    "        \n",
    "    if np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() > 0.01:\n",
    "        print('MASS NOT CONSERVED FOR EMERGENCE VELOCITY')\n",
    "    # Final ice thickness\n",
    "    icethickness_final = volume_final / (xres * yres)\n",
    "    # Emergence velocity\n",
    "    emergence_velocity = icethickness_final - icethickness\n",
    "    return emergence_velocity\n",
    "\n",
    "\n",
    "\n",
    "class GlacFeat:\n",
    "    def __init__(self, feat, glacname_fieldname, glacnum_fieldname):\n",
    "\n",
    "        self.glacname = feat.GetField(glacname_fieldname)\n",
    "        if self.glacname is None:\n",
    "            self.glacname = \"\"\n",
    "        else:\n",
    "            #RGI has some nonstandard characters\n",
    "            #self.glacname = self.glacname.decode('unicode_escape').encode('ascii','ignore')\n",
    "            #glacname = re.sub(r'[^\\x00-\\x7f]',r'', glacname)\n",
    "            self.glacname = re.sub(r'\\W+', '', self.glacname)\n",
    "            self.glacname = self.glacname.replace(\" \", \"\")\n",
    "            self.glacname = self.glacname.replace(\"_\", \"\")\n",
    "            self.glacname = self.glacname.replace(\"/\", \"\")\n",
    "\n",
    "        self.glacnum = feat.GetField(glacnum_fieldname)\n",
    "        fn = feat.GetDefnRef().GetName()\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        self.glacnum = '%0.5f' % float(self.glacnum.split('-')[-1])\n",
    "\n",
    "        if self.glacname:\n",
    "            self.feat_fn = \"%s_%s\" % (self.glacnum, self.glacname)\n",
    "        else:\n",
    "            self.feat_fn = str(self.glacnum)\n",
    "\n",
    "        self.glac_geom_orig = geolib.geom_dup(feat.GetGeometryRef())\n",
    "        self.glac_geom = geolib.geom_dup(self.glac_geom_orig)\n",
    "        #Hack to deal with fact that this is not preserved in geom when loaded from pickle on disk\n",
    "        self.glac_geom_srs_wkt = self.glac_geom.GetSpatialReference().ExportToWkt()\n",
    "\n",
    "        #Attributes written by mb_calc\n",
    "        self.z1 = None\n",
    "        self.z1_hs = None\n",
    "        self.z1_stats = None\n",
    "        self.z1_ela = None\n",
    "        self.z2 = None\n",
    "        self.z2_hs = None\n",
    "        self.z2_stats = None\n",
    "        self.z2_ela = None\n",
    "        self.z2_aspect = None\n",
    "        self.z2_aspect_stats = None\n",
    "        self.z2_slope = None\n",
    "        self.z2_slope_stats = None\n",
    "        self.res = None\n",
    "        self.dhdt = None\n",
    "        self.mb = None\n",
    "        self.mb_mean = None\n",
    "        self.t1 = None\n",
    "        self.t2 = None\n",
    "        self.dt = None\n",
    "        self.t1_mean = None\n",
    "        self.t2_mean = None\n",
    "        self.dt_mean = None\n",
    "\n",
    "        self.H = None\n",
    "        self.H_mean = np.nan\n",
    "        self.vx = None\n",
    "        self.vy = None\n",
    "        self.vm = None\n",
    "        self.vm_mean = np.nan\n",
    "        self.divQ = None\n",
    "        self.emvel = None\n",
    "        self.debris_class = None\n",
    "        self.debris_thick = None\n",
    "        self.debris_thick_mean = np.nan\n",
    "        self.perc_clean = np.nan\n",
    "        self.perc_debris = np.nan\n",
    "        self.perc_pond = np.nan\n",
    "        self.dc_area = None\n",
    "\n",
    "    def geom_srs_update(self, srs=None):\n",
    "        if self.glac_geom.GetSpatialReference() is None:\n",
    "            if srs is None:\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromWkt(self.glac_geom_srs_wkt)\n",
    "            self.glac_geom.AssignSpatialReference(srs)\n",
    "\n",
    "    def geom_attributes(self, srs=None):\n",
    "        self.geom_srs_update()\n",
    "        if srs is not None:\n",
    "            #Should reproject here to equal area, before geom_attributes\n",
    "            #self.glac_geom.AssignSpatialReference(glac_shp_srs)\n",
    "            #self.glac_geom_local = geolib.geom2localortho(self.glac_geom)\n",
    "            geolib.geom_transform(self.glac_geom, srs)\n",
    "\n",
    "        self.glac_geom_extent = geolib.geom_extent(self.glac_geom)\n",
    "        self.glac_area = self.glac_geom.GetArea()\n",
    "        self.glac_area_km2 = self.glac_area / 1E6\n",
    "        self.cx, self.cy = self.glac_geom.Centroid().GetPoint_2D()\n",
    "        \n",
    "        \n",
    "#RGI uses 50 m bins\n",
    "def hist_plot(gf, bin_width=50.0, dz_clim=(-2.0, 2.0), exportcsv=True, csv_ending='', mb_df=None):\n",
    "    #print(\"Generating histograms\")\n",
    "    #Create bins for full range of input data and specified bin width\n",
    "\n",
    "    #NOTE: these counts/areas are for valid pixels only\n",
    "    #Not necessarily a true representation of actual glacier hypsometry\n",
    "    #Need a void-filled DEM for this\n",
    "    if mb_df is not None:\n",
    "        # Align bins with mass balance data\n",
    "        bin_center_min = mb_df.loc[0,'# bin_center_elev_m']\n",
    "        while bin_center_min > gf.z1.min() + bin_width/2:\n",
    "            bin_center_min -= mb_bin_size\n",
    "        bin_center_max = mb_df['# bin_center_elev_m'].values[-1]\n",
    "        while bin_center_max < gf.z1.max():\n",
    "            bin_center_max += mb_bin_size    \n",
    "        z_bin_centers = np.arange(bin_center_min, bin_center_max + mb_bin_size/2, mb_bin_size)\n",
    "        z_bin_edges = np.arange(bin_center_min - mb_bin_size / 2, bin_center_max + mb_bin_size, mb_bin_size)\n",
    "    else:\n",
    "        z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "        \n",
    "    #Need to compress here, otherwise histogram uses masked values!\n",
    "    z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "    z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "    #RGI standard is integer thousandths of glaciers total area\n",
    "    #Should check to make sure sum of bin areas equals total area\n",
    "    #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "    z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #If we only have one elevation grid with dhdt\n",
    "    if gf.z2 is not None:\n",
    "        z2_bin_counts, z2_bin_edges = np.histogram(gf.z2.compressed(), bins=z_bin_edges)\n",
    "        z2_bin_areas = z2_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #z2_bin_areas_perc = 100. * z2_bin_areas / np.sum(z2_bin_areas)\n",
    "        z2_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "    else:\n",
    "        z2_bin_counts = z1_bin_counts\n",
    "        z2_bin_edges = z1_bin_edges\n",
    "        z2_bin_areas = z1_bin_areas\n",
    "        z2_bin_areas_perc = z1_bin_areas_perc\n",
    "        \n",
    "    if gf.dc_area is not None:\n",
    "#         z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "#         #Need to compress here, otherwise histogram uses masked values!\n",
    "#         z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "#         z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "#         #RGI standard is integer thousandths of glaciers total area\n",
    "#         #Should check to make sure sum of bin areas equals total area\n",
    "#         #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "#         z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "        \n",
    "# #         dc_bin_edges, dc_bin_centers = malib.get_bins(gf.dc_area, bin_width)\n",
    "        dc_bin_counts, dc_bin_edges = np.histogram(gf.dc_area.compressed(), bins=z_bin_edges)\n",
    "        dc_bin_areas = dc_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #RGI standard is integer thousandths of glaciers total area\n",
    "        dc_bin_areas_perc = 100. * (dc_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #Create arrays to store output\n",
    "    slope_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    slope_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.dhdt is not None:\n",
    "        mb_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        np.ma.set_fill_value(mb_bin_med, np.nan)\n",
    "        mb_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_count = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.vm is not None:\n",
    "        vm_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        vm_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.H is not None:\n",
    "        H_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        H_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.emvel is not None:\n",
    "        emvel_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.debris_class is not None:\n",
    "#         perc_clean = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_debris = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_pond = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_clean_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_debris_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_pond_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "\n",
    "#         gf.dhdt_clean = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 1).data))\n",
    "#         gf.dhdt_debris = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 2).data))\n",
    "#         gf.dhdt_pond = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 3).data))\n",
    "\n",
    "    if gf.debris_thick_ts is not None:\n",
    "        debris_thick_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        meltfactor_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        meltfactor_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "\n",
    "    #Bin sample count must be greater than this value\n",
    "    min_bin_samp_count = 9\n",
    "\n",
    "    #Loop through each bin and extract stats\n",
    "    idx = np.digitize(gf.z1, z_bin_edges)\n",
    "    for bin_n in range(z_bin_centers.size):\n",
    "        if gf.dhdt is not None:\n",
    "            mb_bin_samp = gf.mb_map[(idx == bin_n+1)]\n",
    "            if mb_bin_samp.count() > min_bin_samp_count:\n",
    "                mb_bin_med[bin_n] = malib.fast_median(mb_bin_samp)\n",
    "                mb_bin_mad[bin_n] = malib.mad(mb_bin_samp)\n",
    "                mb_bin_mean[bin_n] = mb_bin_samp.mean()\n",
    "                mb_bin_std[bin_n] = mb_bin_samp.std()\n",
    "            dhdt_bin_samp = gf.dhdt[(idx == bin_n+1)]\n",
    "            if dhdt_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_bin_med[bin_n] = malib.fast_median(dhdt_bin_samp)\n",
    "                dhdt_bin_mad[bin_n] = malib.mad(dhdt_bin_samp)\n",
    "                dhdt_bin_mean[bin_n] = dhdt_bin_samp.mean()\n",
    "                dhdt_bin_std[bin_n] = dhdt_bin_samp.std()\n",
    "                dhdt_bin_count[bin_n] = dhdt_bin_samp.count()\n",
    "        if gf.debris_thick is not None:\n",
    "            debris_thick_bin_samp = gf.debris_thick[(idx == bin_n+1)]\n",
    "            if debris_thick_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_med[bin_n] = malib.fast_median(debris_thick_bin_samp)\n",
    "                debris_thick_mad[bin_n] = malib.mad(debris_thick_bin_samp)\n",
    "        \n",
    "        if gf.debris_thick_ts is not None:\n",
    "            debris_thick_ts_bin_samp = gf.debris_thick_ts[(idx == bin_n+1)]\n",
    "            if debris_thick_ts_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_ts_med[bin_n] = malib.fast_median(debris_thick_ts_bin_samp)\n",
    "                debris_thick_ts_mad[bin_n] = malib.mad(debris_thick_ts_bin_samp)\n",
    "        if gf.meltfactor_ts is not None:\n",
    "            meltfactor_ts_bin_samp = gf.meltfactor_ts[(idx == bin_n+1)]\n",
    "            if meltfactor_ts_bin_samp.size > min_bin_samp_count:\n",
    "                meltfactor_ts_med[bin_n] = malib.fast_median(meltfactor_ts_bin_samp)\n",
    "                meltfactor_ts_mad[bin_n] = malib.mad(meltfactor_ts_bin_samp)\n",
    "        \n",
    "        if gf.debris_class is not None:\n",
    "            debris_class_bin_samp = gf.debris_class[(idx == bin_n+1)]\n",
    "            dhdt_clean_bin_samp = gf.dhdt_clean[(idx == bin_n+1)]\n",
    "            dhdt_debris_bin_samp = gf.dhdt_debris[(idx == bin_n+1)]\n",
    "            dhdt_pond_bin_samp = gf.dhdt_pond[(idx == bin_n+1)]\n",
    "            if debris_class_bin_samp.count() > min_bin_samp_count:\n",
    "                perc_clean[bin_n] = 100. * (debris_class_bin_samp == 1).sum()/debris_class_bin_samp.count()\n",
    "                perc_debris[bin_n] = 100. * (debris_class_bin_samp == 2).sum()/debris_class_bin_samp.count()\n",
    "                perc_pond[bin_n] = 100. * (debris_class_bin_samp == 3).sum()/debris_class_bin_samp.count()\n",
    "            if dhdt_clean_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_clean_bin_med[bin_n] = malib.fast_median(dhdt_clean_bin_samp)\n",
    "            if dhdt_debris_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_debris_bin_med[bin_n] = malib.fast_median(dhdt_debris_bin_samp)\n",
    "            if dhdt_pond_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_pond_bin_med[bin_n] = malib.fast_median(dhdt_pond_bin_samp)\n",
    "        if gf.vm is not None:\n",
    "            vm_bin_samp = gf.vm[(idx == bin_n+1)]\n",
    "            if vm_bin_samp.size > min_bin_samp_count:\n",
    "                vm_bin_med[bin_n] = malib.fast_median(vm_bin_samp)\n",
    "                vm_bin_mad[bin_n] = malib.mad(vm_bin_samp)\n",
    "        if gf.H is not None:\n",
    "            H_bin_samp = gf.H[(idx == bin_n+1)]\n",
    "            if H_bin_samp.size > min_bin_samp_count:\n",
    "                H_bin_mean[bin_n] = H_bin_samp.mean()\n",
    "                H_bin_std[bin_n] = H_bin_samp.std()\n",
    "        if gf.emvel is not None:\n",
    "            emvel_bin_samp = gf.emvel[(idx == bin_n+1)]\n",
    "            if emvel_bin_samp.size > min_bin_samp_count:\n",
    "                emvel_bin_mean[bin_n] = emvel_bin_samp.mean()\n",
    "                emvel_bin_std[bin_n] = emvel_bin_samp.std()\n",
    "                emvel_bin_med[bin_n] = malib.fast_median(emvel_bin_samp)\n",
    "                emvel_bin_mad[bin_n] = malib.mad(emvel_bin_samp)\n",
    "        slope_bin_samp = gf.z1_slope[(idx == bin_n+1)]\n",
    "        if slope_bin_samp.size > min_bin_samp_count:\n",
    "            slope_bin_med[bin_n] = malib.fast_median(slope_bin_samp)\n",
    "            slope_bin_mad[bin_n] = malib.mad(slope_bin_samp)\n",
    "        aspect_bin_samp = gf.z1_aspect[(idx == bin_n+1)]\n",
    "        if aspect_bin_samp.size > min_bin_samp_count:\n",
    "            aspect_bin_med[bin_n] = malib.fast_median(aspect_bin_samp)\n",
    "            aspect_bin_mad[bin_n] = malib.mad(aspect_bin_samp)\n",
    "\n",
    "    if gf.dhdt is not None:\n",
    "        dhdt_bin_areas = dhdt_bin_count * gf.res[0] * gf.res[1] / 1E6\n",
    "        #dhdt_bin_areas_perc = 100. * dhdt_bin_areas / np.sum(dhdt_bin_areas)\n",
    "        dhdt_bin_areas_perc = 100. * (dhdt_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    outbins_header = 'bin_center_elev_m, z1_bin_count_valid, z1_bin_area_valid_km2, z1_bin_area_perc, z2_bin_count_valid, z2_bin_area_valid_km2, z2_bin_area_perc, slope_bin_med, aspect_bin_med'\n",
    "    fmt = '%0.1f, %0.0f, %0.3f, %0.2f, %0.0f, %0.3f, %0.2f, %0.2f, %0.2f'\n",
    "    outbins = [z_bin_centers, z1_bin_counts, z1_bin_areas, z1_bin_areas_perc, z2_bin_counts, z2_bin_areas, z2_bin_areas_perc, slope_bin_med, aspect_bin_med]\n",
    "    if gf.dhdt is not None:\n",
    "        outbins_header += ', dhdt_bin_count, dhdt_bin_area_valid_km2, dhdt_bin_area_perc, dhdt_bin_med_ma, dhdt_bin_mad_ma, dhdt_bin_mean_ma, dhdt_bin_std_ma, mb_bin_med_mwea, mb_bin_mad_mwea, mb_bin_mean_mwea, mb_bin_std_mwea'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([dhdt_bin_count, dhdt_bin_areas, dhdt_bin_areas_perc, dhdt_bin_med, dhdt_bin_mad, dhdt_bin_mean, dhdt_bin_std, \\\n",
    "                        mb_bin_med, mb_bin_mad, mb_bin_mean, mb_bin_std])\n",
    "    if gf.dc_area is not None:\n",
    "        outbins_header += ', dc_bin_count_valid, dc_bin_area_valid_km2, dc_bin_area_perc'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f'\n",
    "        outbins.extend([dc_bin_counts, dc_bin_areas, dc_bin_areas_perc])\n",
    "#         outbins.extend([z1_bin_counts, z1_bin_areas, z1_bin_areas_perc])\n",
    "        \n",
    "        \n",
    "    if gf.debris_thick is not None:\n",
    "        outbins_header += ', debris_thick_med_m, debris_thick_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_med[debris_thick_med == -(np.inf)] = 0.00\n",
    "        debris_thick_mad[debris_thick_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_med, debris_thick_mad])\n",
    "    \n",
    "    if gf.debris_thick_ts is not None:\n",
    "        outbins_header += ',debris_thick_ts_med_m,debris_thick_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_ts_med[debris_thick_ts_med == -(np.inf)] = 0.00\n",
    "        debris_thick_ts_mad[debris_thick_ts_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_ts_med, debris_thick_ts_mad])\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        outbins_header += ',meltfactor_ts_med_m,meltfactor_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        meltfactor_ts_med[meltfactor_ts_med == -(np.inf)] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med > 1] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med <= 0] = 1\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad == -(np.inf)] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad > 1] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad <= 0] = 0\n",
    "        outbins.extend([meltfactor_ts_med, meltfactor_ts_mad])\n",
    "    \n",
    "    if gf.debris_class is not None:\n",
    "        outbins_header += ', perc_debris, perc_pond, perc_clean, dhdt_debris_med, dhdt_pond_med, dhdt_clean_med'\n",
    "        fmt += ', %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([perc_debris, perc_pond, perc_clean, dhdt_debris_bin_med, dhdt_pond_bin_med, dhdt_clean_bin_med])\n",
    "    if gf.vm is not None:\n",
    "        outbins_header += ', vm_med, vm_mad'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([vm_bin_med, vm_bin_mad])\n",
    "    if gf.H is not None:\n",
    "        outbins_header += ', H_mean, H_std'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([H_bin_mean, H_bin_std])\n",
    "#         outbins_header += ', H_mean, H_std, emvel_mean, emvel_std'\n",
    "#         fmt += ', %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "#         outbins.extend([H_bin_mean, H_bin_std, emvel_bin_mean, emvel_bin_std])\n",
    "\n",
    "    if gf.emvel is not None:\n",
    "        outbins_header += ', emvel_mean, emvel_std, emvel_med, emvel_mad'\n",
    "        fmt += ', %0.3f, %0.3f, %0.3f, %0.3f'\n",
    "        outbins.extend([emvel_bin_mean, emvel_bin_std, emvel_bin_med, emvel_bin_mad])\n",
    "    \n",
    "    outbins = np.ma.array(outbins).T.astype('float32')\n",
    "    np.ma.set_fill_value(outbins, np.nan)\n",
    "    outbins = outbins.filled(np.nan)\n",
    "    \n",
    "    outbins_df = pd.DataFrame(outbins, columns=outbins_header.split(','))\n",
    "    outbins_df['debris_perc'] = outbins_df[' dc_bin_count_valid'] / outbins_df[' z1_bin_count_valid'] * 100\n",
    "    \n",
    "    if mb_df is not None:\n",
    "        # ADD MASS BALANCE DATA\n",
    "        mb_df = mb_df[np.isfinite(mb_df['# bin_center_elev_m'])]\n",
    "        mb_df.reset_index(inplace=True, drop=True)\n",
    "        # start index for merge\n",
    "        if mb_df.loc[0,'# bin_center_elev_m'] >= outbins_df.loc[0,'bin_center_elev_m']:\n",
    "            mb_df_idx1 = 0\n",
    "            outbins_idx1 = np.where(outbins_df['bin_center_elev_m'] == mb_df.loc[0,'# bin_center_elev_m'])[0][0]\n",
    "        else:\n",
    "            outbins_idx1 = 0\n",
    "            mb_df_idx1 = np.where(outbins_df.loc[0,'bin_center_elev_m'] == mb_df['# bin_center_elev_m'])[0][0]\n",
    "    #     print('idx1:', \n",
    "    #           '\\noutbins:', outbins_idx1, outbins_df.loc[outbins_idx1,'bin_center_elev_m'],\n",
    "    #           '\\ndfbins:', mb_df_idx1, mb_df.loc[mb_df_idx1,'# bin_center_elev_m'])\n",
    "        # end index for merge\n",
    "        if outbins_df.loc[outbins_df.shape[0]-1,'bin_center_elev_m'] >= mb_df.loc[mb_df.shape[0]-1,'# bin_center_elev_m']:\n",
    "            outbins_idx2 = np.where(outbins_df['bin_center_elev_m'] == mb_df.loc[mb_df.shape[0]-1,'# bin_center_elev_m'])[0][0]\n",
    "            mb_df_idx2 = mb_df.shape[0]-1\n",
    "        else:\n",
    "            outbins_idx2 = outbins_df.shape[0]-1\n",
    "            mb_df_idx2 = np.where(outbins_df.loc[outbins_df.shape[0]-1,'bin_center_elev_m'] == mb_df['# bin_center_elev_m'])[0][0]\n",
    "    #     print('idx2:', \n",
    "    #           '\\noutbins:', outbins_idx2, outbins_df.loc[outbins_idx2,'bin_center_elev_m'],\n",
    "    #           '\\ndfbins:', mb_df_idx2, mb_df.loc[mb_df_idx2,'# bin_center_elev_m'])\n",
    "        outbins_df[' mb_bin_mean_mwea'] = np.nan\n",
    "        outbins_df[' mb_bin_std_mwea'] = np.nan\n",
    "        outbins_df[' mb_bin_area_valid_km2'] = np.nan\n",
    "        outbins_df.loc[outbins_idx1:outbins_idx2+1,' mb_bin_mean_mwea'] = mb_df.loc[mb_df_idx1:mb_df_idx2+1,' mb_bin_mean_mwea']\n",
    "        outbins_df.loc[outbins_idx1:outbins_idx2+1,' mb_bin_std_mwea'] = mb_df.loc[mb_df_idx1:mb_df_idx2+1,' mb_bin_std_mwea']\n",
    "        outbins_df.loc[outbins_idx1:outbins_idx2+1,' mb_bin_area_valid_km2'] = mb_df.loc[mb_df_idx1:mb_df_idx2+1,' z1_bin_area_valid_km2']\n",
    "        try:\n",
    "            outbins_df['startyear'] = mb_df.loc[mb_df_idx1,'startyear']\n",
    "            outbins_df['endyear'] = mb_df.loc[mb_df_idx1,'endyear']\n",
    "        except:\n",
    "            outbins_df['startyear'] = 2000\n",
    "            outbins_df['endyear'] = 2012\n",
    "    \n",
    "    if exportcsv:\n",
    "        if int(gf.feat_fn.split('.')[0]) < 10:\n",
    "            outbins_fullfn = os.path.join(input.mb_binned_fp, gf.feat_fn[0:7] + csv_ending)\n",
    "        else:\n",
    "            outbins_fullfn = os.path.join(input.mb_binned_fp, gf.feat_fn[0:8] + csv_ending)\n",
    "        outbins_df.to_csv(outbins_fullfn, index=False)\n",
    "#         np.savetxt(outbins_fn, outbins, fmt=fmt, delimiter=',', header=outbins_header)\n",
    "    \n",
    "    outbins_df = pd.DataFrame(outbins, columns=outbins_header.split(','))\n",
    "    \n",
    "    return outbins_df, z_bin_edges\n",
    "#     return z_bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\"\"\"\n",
    "Compute debris thickness through sub-debris and temperature inversion methods\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy import ndimage\n",
    "import xarray as xr\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "from pygeotools.lib import malib, warplib, geolib, iolib, timelib\n",
    "# from imview.lib import pltlib\n",
    "\n",
    "import globaldebris_input as input\n",
    "\n",
    "csv_ending = '_mb_bins_wdc_emvel_offset.csv'\n",
    "verbose = False\n",
    "close_fig = True\n",
    "extra_layers = True\n",
    "\n",
    "input.roi = '01'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golive_fp = '/Users/davidrounce/Documents/Dave_Rounce/Satellite_Images/GoLIVE/p064_r017-Kennicott/'\n",
    "tdays = 32\n",
    "\n",
    "golive_fns = []\n",
    "for i in os.listdir(golive_fp):\n",
    "    if i.endswith('.nc') and i.endswith('_nrt.nc') == False:\n",
    "        if int(i.split('_')[3]) == tdays:\n",
    "            golive_fns.append(golive_fp + i)\n",
    "    \n",
    "# vx_stack = None\n",
    "# for nfn, fn in enumerate(golive_fns):\n",
    "# #for nfn, fn in enumerate([golive_fns[0]]):\n",
    "# #for nfn, fn in enumerate(golive_fns[0:2]):\n",
    "#     ds = xr.open_dataset(golive_fp + fn)\n",
    "len(golive_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "proj_fn = golive_fns[0]\n",
    "ds = gdal.Open(proj_fn)\n",
    "prj = ds.GetProjection()\n",
    "srs = osr.SpatialReference(wkt=prj)\n",
    "aea_srs = srs\n",
    "\n",
    "\n",
    "\n",
    "ds_list = warplib.memwarp_multi_fn(golive_fns, res='min', \\\n",
    "                extent='intersection', t_srs='first', verbose=verbose, \\\n",
    "                r='cubic')\n",
    "ds_dict = dict(zip(np.arange(0,len(golive_fns)), ds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x11bb16b10> >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-2c884cf10b09>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-2c884cf10b09>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    gdal.Translate('test.tiff', golive_fns[0]:vv)\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gdal.Translate('test.tiff', golive_fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(golive_fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109 glaciers in region 1 are included in this model run: ['00006', '00013', '00027', '00033', '00035', '00037', '00038', '00040', '00041', '00042', '00044', '00045', '00046', '00140', '00148', '00187', '00242', '00312', '00336', '00348', '00351', '00399', '00409', '00426', '00434', '00436', '00537', '00544', '00556', '00557', '00558', '00560', '00561', '00565', '00566', '00569', '00570', '00571', '00572', '00574', '00576', '00578', '00579', '00581', '00582', '00584', '00600', '00660', '00670', '00675'] and more\n",
      "This study is focusing on 1109 glaciers in region [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([623]),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debris cover extent shapefile with statistics\n",
    "dc_shp = gpd.read_file(input.debriscover_fp + input.debriscover_fn_dict[input.roi])\n",
    "dc_shp = dc_shp.sort_values(by=['RGIId'])\n",
    "\n",
    "# Subset by percent debris-covered or debris-covered area\n",
    "dc_shp_subset = dc_shp[((dc_shp['DC_Area__1'] > input.dc_percarea_threshold) | \n",
    "                        (dc_shp['DC_Area_v2'] / 1e6 > input.dc_area_threshold))\n",
    "                        & (dc_shp['Area'] > input.min_glac_area)].copy()\n",
    "dc_shp_subset.reset_index(inplace=True, drop=True)\n",
    "dc_shp_subset['CenLon_360'] = dc_shp_subset['CenLon']\n",
    "dc_shp_subset.loc[dc_shp_subset['CenLon_360'] < 0, 'CenLon_360'] = 360 + dc_shp_subset.loc[dc_shp_subset['CenLon_360'] < 0, 'CenLon_360']\n",
    "dc_shp_subset\n",
    "\n",
    "rgiid_list = [x.split('-')[1] for x in dc_shp_subset['RGIId'].values]\n",
    "main_glac_rgi = input.selectglaciersrgitable(rgiid_list)\n",
    "\n",
    "np.where(main_glac_rgi.RGIId == 'RGI60-01.15645')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 623 RGI60-01.15645\n"
     ]
    }
   ],
   "source": [
    "# for nglac, glac_idx in enumerate(main_glac_rgi.index.values):\n",
    "# for nglac, glac_idx in enumerate(main_glac_rgi.index.values[[2306]]):\n",
    "# for nglac, glac_idx in enumerate([main_glac_rgi.index.values[502]]):\n",
    "for nglac, glac_idx in enumerate([main_glac_rgi.index.values[623]]):\n",
    "    glac_str = main_glac_rgi.loc[glac_idx,'rgino_str']\n",
    "    rgiid = main_glac_rgi.loc[glac_idx,'RGIId']\n",
    "    \n",
    "    print(nglac, glac_idx, rgiid)\n",
    "    \n",
    "    out_csv_fn = os.path.join(input.mb_binned_fp, glac_str + csv_ending)\n",
    "    if verbose:\n",
    "        print('output_fn:', out_csv_fn)\n",
    "\n",
    "    if not os.path.exists(out_csv_fn):\n",
    "    \n",
    "        region = glac_str.split('.')[0]\n",
    "        \n",
    "        # ===== Project shapefile =====\n",
    "        huss_dir = input.huss_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "        huss_fn = input.huss_fn_sample.replace('XXXX',glac_str)\n",
    "\n",
    "        proj_fn = os.path.join(huss_dir, huss_fn) # THIS PROJECTION IS KEY!\n",
    "        ds = gdal.Open(proj_fn)\n",
    "        prj = ds.GetProjection()\n",
    "        srs = osr.SpatialReference(wkt=prj)\n",
    "        aea_srs = srs\n",
    "        \n",
    "        # Shape layer processing\n",
    "        # If projected shapefile already exists, then skip projection\n",
    "        glac_shp_proj_fn = (input.glac_shp_proj_fp + glac_str + '_crs' + \n",
    "                            str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp')\n",
    "        dc_shp_proj_fn = (input.glac_shp_proj_fp + glac_str + '_dc_crs' + \n",
    "                          str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp')\n",
    "        if os.path.exists(glac_shp_proj_fn) == False:\n",
    "            glac_shp_init = gpd.read_file(input.glac_shp_fn_dict[region])\n",
    "            if verbose:\n",
    "                print('Shp init crs:', glac_shp_init.crs)\n",
    "            glac_shp_single = glac_shp_init[glac_shp_init['RGIId'] == rgiid]\n",
    "            glac_shp_single = glac_shp_single.reset_index()\n",
    "            glac_shp_proj = glac_shp_single.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "            glac_shp_proj.to_file(glac_shp_proj_fn)\n",
    "\n",
    "        if os.path.exists(dc_shp_proj_fn) == False:\n",
    "            dc_shp_init = gpd.read_file(input.debriscover_fp + input.debriscover_fn_dict[input.roi])\n",
    "            dc_shp_single = dc_shp_init[dc_shp_init['RGIId'] == rgiid]\n",
    "            dc_shp_single = dc_shp_single.reset_index()\n",
    "            dc_shp_proj = dc_shp_single.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "            dc_shp_proj.to_file(dc_shp_proj_fn)\n",
    "\n",
    "        glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "        glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "        #This should be contained in features\n",
    "        glac_shp_srs = glac_shp_lyr.GetSpatialRef()\n",
    "        feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "        if verbose:\n",
    "            print(\"Input glacier polygon count: %i\" % feat_count)\n",
    "\n",
    "        dc_shp_ds = ogr.Open(dc_shp_proj_fn, 0)\n",
    "        dc_shp_lyr = dc_shp_ds.GetLayer()\n",
    "        #This should be contained in features\n",
    "        dc_shp_srs = dc_shp_lyr.GetSpatialRef()\n",
    "        feat_count = dc_shp_lyr.GetFeatureCount()\n",
    "        if verbose:\n",
    "            print(\"Input glacier polygon count (debris cover): %i\" % feat_count)\n",
    "            \n",
    "        # Load DEM\n",
    "        z1_dir = input.z1_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "        z1_fn = input.z1_fn_sample.replace('XXXX',glac_str)\n",
    "        z1_ds = gdal.Open(z1_dir + z1_fn)\n",
    "        z1_int_geom = geolib.ds_geom_intersection([z1_ds, z1_ds], t_srs=glac_shp_srs)\n",
    "\n",
    "        glacname_fieldname = \"Name\"\n",
    "        glacnum_fieldname = \"RGIId\"\n",
    "        glacnum_fmt = '%08.5f'\n",
    "\n",
    "        for n, feat in enumerate(glac_shp_lyr):\n",
    "            gf = GlacFeat(feat, glacname_fieldname, glacnum_fieldname)\n",
    "            if verbose:\n",
    "                print(\"%i of %i: %s\" % (n+1, feat_count, gf.feat_fn))\n",
    "            #NOTE: Input must be in projected coordinate system, ideally equal area\n",
    "            #Should check this and reproject\n",
    "            gf.geom_attributes(srs=aea_srs)\n",
    "\n",
    "        if verbose:\n",
    "            print(gf.feat_fn)\n",
    "            \n",
    "            \n",
    "        fn_dict = OrderedDict()\n",
    "        #We at least want to warp the two input DEMs\n",
    "        fn_dict['z1'] = os.path.join(z1_dir, z1_fn)\n",
    "\n",
    "        if extra_layers and (gf.glac_area_km2 > input.min_glac_area_writeout):\n",
    "            if verbose:\n",
    "                print(gf.glacnum)\n",
    "\n",
    "            # Ice thickness data\n",
    "            ice_thick_fn = os.path.join(huss_dir, huss_fn)\n",
    "            if os.path.exists(ice_thick_fn):\n",
    "                fn_dict['ice_thick'] = ice_thick_fn\n",
    "\n",
    "            if os.path.exists(input.v_dir + input.vx_fn_dict[input.roi]):\n",
    "                fn_dict['vx'] = input.v_dir + input.vx_fn_dict[input.roi]\n",
    "                fn_dict['vy'] = input.v_dir + input.vy_fn_dict[input.roi]\n",
    "\n",
    "            if os.path.exists(input.ts_fp + input.ts_fn_dict[input.roi]):\n",
    "                fn_dict['ts'] = input.ts_fp + input.ts_fn_dict[input.roi]\n",
    "\n",
    "\n",
    "        #Expand extent to include buffered region around glacier polygon\n",
    "        warp_extent = geolib.pad_extent(gf.glac_geom_extent, width=input.buff_dist)\n",
    "        if verbose:\n",
    "            print(\"Expanding extent\")\n",
    "            print(gf.glac_geom_extent)\n",
    "            print(warp_extent)\n",
    "            print(aea_srs)\n",
    "        \n",
    "        #Warp everything to common res/extent/proj\n",
    "        z1_gt = gdal.Open(fn_dict['z1']).GetGeoTransform()\n",
    "        z1_res = np.min([z1_gt[1], -z1_gt[5]])\n",
    "        ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res=z1_res, \\\n",
    "                extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "                r='cubic')\n",
    "        ds_dict = dict(zip(fn_dict.keys(), ds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if verbose:\n",
    "#             print(ds_list)\n",
    "#             print(fn_dict.keys())\n",
    "            \n",
    "        \n",
    "#         #Prepare mask for all glaciers within buffered area, not just the current glacier polygon\n",
    "#         glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "#         glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "#         dc_shp_ds = ogr.Open(dc_shp_proj_fn, 0)\n",
    "#         dc_shp_lyr = dc_shp_ds.GetLayer()\n",
    "\n",
    "#         #Get global glacier mask\n",
    "#         #Want this to be True over ALL glacier surfaces, not just the current polygon\n",
    "#         glac_shp_lyr_mask = geolib.lyr2mask(glac_shp_lyr, ds_dict['ice_thick'])\n",
    "#         dc_shp_lyr_mask = geolib.lyr2mask(dc_shp_lyr, ds_dict['ice_thick'])\n",
    "\n",
    "#         #Create buffer around glacier polygon\n",
    "#         glac_geom_buff = gf.glac_geom.Buffer(input.buff_dist)\n",
    "#         #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "#         glac_geom_buff_mask = geolib.geom2mask(glac_geom_buff, ds_dict['ice_thick'])\n",
    "\n",
    "#         # ds masks\n",
    "#         ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "#         dem1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "#         dems_mask = dem1.mask\n",
    "#         if verbose:\n",
    "#             print('list of datasets:', len(ds_list_masked), fn_dict.values())\n",
    "\n",
    "#         #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "#         static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "#         static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "        \n",
    "#         if 'z1' in ds_dict:\n",
    "#             #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "#             glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "#             gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']))\n",
    "#             #gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "\n",
    "#             # Debris cover\n",
    "#             dc_mask = np.ma.mask_or(dc_shp_lyr_mask, glac_geom_mask)\n",
    "#             gf.dc_area = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=dc_mask)\n",
    "\n",
    "#             # Check if DEM has huge errors or not - replace if necessary\n",
    "#             if input.roi in ['01']:\n",
    "\n",
    "#                 gf.z1_check = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "#                 if gf.z1_check.min() < 0:\n",
    "\n",
    "#                     # Add backup DEM for regions with known poor quality (ex. Alaska)\n",
    "#                     print('switching DEMs')\n",
    "#                     fn_dict['z1_backup'] = input.z1_backup_dict[input.roi]\n",
    "#                     # Warp everything to common res/extent/proj (a second time)\n",
    "#                     ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res=z1_res, \\\n",
    "#                             extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "#                             r='cubic')\n",
    "#                     ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "\n",
    "#                     if verbose:\n",
    "#                         print(ds_list)\n",
    "#                         print(fn_dict.keys())\n",
    "\n",
    "#                     # ds masks\n",
    "#                     ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "#                     dem1 = np.ma.masked_less_equal(ds_list_masked[-1], 0)\n",
    "#                     dems_mask = dem1.mask\n",
    "#                     if verbose:\n",
    "#                         print('list of datasets:', len(ds_list_masked), fn_dict.values())\n",
    "\n",
    "#                     #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "#                     static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "#                     static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "\n",
    "#                     #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "#                     glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1_backup'])\n",
    "#                     gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1_backup']), mask=glac_geom_mask)\n",
    "#                     #gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "\n",
    "#                     # Debris cover\n",
    "#                     dc_mask = np.ma.mask_or(dc_shp_lyr_mask, glac_geom_mask)\n",
    "#                     gf.dc_area = np.ma.array(iolib.ds_getma(ds_dict['z1_backup']), mask=dc_mask)\n",
    "\n",
    "\n",
    "#             if verbose:\n",
    "#                 print('\\n\\n# z1 pixels:', gf.z1.count(), '\\n')\n",
    "#             if gf.z1.count() == 0:\n",
    "#                 if verbose:\n",
    "#                     print(\"No z1 pixels\")\n",
    "#         else:\n",
    "#             print(\"Unable to load z1 ds\")\n",
    "            \n",
    "#         # ===== ADD VARIOUS LAYERS TO gf =====\n",
    "#         if nglac == 0:\n",
    "#             print('\\n\\nHACK TO BYPASS VALID AREA\\n\\n')\n",
    "#         gf.valid_area_perc = 100\n",
    "\n",
    "#         if gf.valid_area_perc < (100. * input.min_valid_area_perc):\n",
    "#             if verbose:\n",
    "#                 print(\"Not enough valid pixels. %0.1f%% percent of glacier polygon area\" % (gf.valid_area_perc))\n",
    "#         #     return None\n",
    "\n",
    "#         else:\n",
    "#             #Filter dz - throw out abs differences >150 m\n",
    "\n",
    "#             #Compute dz, volume change, mass balance and stats\n",
    "#             gf.z1_stats = malib.get_stats(gf.z1)\n",
    "#             z1_elev_med = gf.z1_stats[5]\n",
    "#             z1_elev_min, z1_elev_max = malib.calcperc(gf.z1, (0.1, 99.9))\n",
    "\n",
    "#             #Caluclate stats for aspect and slope using z2\n",
    "#             #Requires GDAL 2.1+\n",
    "#             gf.z1_aspect = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='aspect', returnma=True), mask=glac_geom_mask)\n",
    "#             gf.z1_aspect_stats = malib.get_stats(gf.z1_aspect)\n",
    "#             z1_aspect_med = gf.z1_aspect_stats[5]\n",
    "#             gf.z1_slope = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='slope', returnma=True), mask=glac_geom_mask)\n",
    "#             gf.z1_slope_stats = malib.get_stats(gf.z1_slope)\n",
    "#             z1_slope_med = gf.z1_slope_stats[5]\n",
    "\n",
    "#             #Can estimate ELA values computed from hypsometry and typical AAR\n",
    "#             #For now, assume ELA is mean\n",
    "#             gf.z1_ela = None\n",
    "#             gf.z1_ela = gf.z1_stats[3]\n",
    "#             #Note: in theory, the ELA should get higher with mass loss\n",
    "#             #In practice, using mean and same polygon, ELA gets lower as glacier surface thins\n",
    "\n",
    "#             # copy for Ts because it change the mask otherwise and messes up binned statistics for whole glacier\n",
    "#             glac_geom_mask_copy = glac_geom_mask.copy()\n",
    "            \n",
    "#             if extra_layers and (gf.glac_area_km2 > input.min_glac_area_writeout):\n",
    "#                 if 'ice_thick' in ds_dict:\n",
    "#                     #Load ice thickness\n",
    "#                     gf.H = np.ma.array(iolib.ds_getma(ds_dict['ice_thick']), mask=glac_geom_mask)\n",
    "#                     gf.H_mean = gf.H.mean()\n",
    "#                     if verbose:\n",
    "#                         print('mean ice thickness [m]:', gf.H_mean)\n",
    "\n",
    "#                 if 'vx' in ds_dict and 'vy' in ds_dict:\n",
    "#                     #Load surface velocity maps\n",
    "#                     gf.vx = np.ma.array(iolib.ds_getma(ds_dict['vx']), mask=glac_geom_mask)\n",
    "#                     gf.vy = np.ma.array(iolib.ds_getma(ds_dict['vy']), mask=glac_geom_mask)\n",
    "#                     gf.vm = np.ma.sqrt(gf.vx**2 + gf.vy**2)\n",
    "#                     gf.vm_mean = gf.vm.mean()\n",
    "#                     if verbose:\n",
    "#                         print('mean velocity [m/s]:', gf.vm_mean)\n",
    "\n",
    "#                     if gf.H is not None:\n",
    "#                         #Compute flux\n",
    "#                         gf.Q = gf.H * input.v_col_f * np.array([gf.vx, gf.vy])\n",
    "#                         #Note: np.gradient returns derivatives relative to axis number, so (y, x) in this case\n",
    "#                         #Want x-derivative of x component\n",
    "#                         gf.divQ = np.gradient(gf.Q[0])[1] + np.gradient(gf.Q[1])[0]\n",
    "\n",
    "#         #                 gf.divQ = gf.H*(np.gradient(v_col_f*gf.vx)[1] + np.gradient(v_col_f*gf.vy)[0]) \\\n",
    "#         #                         + v_col_f*gf.vx*(np.gradient(gf.H)[1]) + v_col_f*gf.vy*(np.gradient(gf.H)[0])\n",
    "\n",
    "#                         #Should smooth divQ, better handling of data gaps\n",
    "\n",
    "#                 if 'ts' in ds_dict:\n",
    "#                     #Load surface temperature maps\n",
    "#                     gf.ts = np.ma.array(iolib.ds_getma(ds_dict['ts']), mask=glac_geom_mask_copy)\n",
    "#                     gf.ts.mask = np.ma.mask_or(glac_geom_mask, np.ma.getmask(np.ma.masked_array(gf.ts.data, np.isnan(gf.ts.data))))\n",
    "#                 else:\n",
    "#                     gf.ts = None\n",
    "\n",
    "#                 if 'debris_thick_ts' in ds_dict:\n",
    "#                     # Load debris thickness map\n",
    "#                     gf.debris_thick_ts = np.ma.array(iolib.ds_getma(ds_dict['debris_thick_ts']), mask=glac_geom_mask_copy)\n",
    "#                     gf.meltfactor_ts = None\n",
    "#                 else:\n",
    "#                     gf.debris_thick_ts = None\n",
    "#                     gf.meltfactor_ts = None\n",
    "\n",
    "#             gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "#             if verbose:\n",
    "#                 print('Area [km2]:', gf.glac_area / 1e6)\n",
    "#                 print('-------------------------------')\n",
    "                \n",
    "                \n",
    "#             gf.z1 = np.ma.array(gf.z1, mask=glac_geom_mask)\n",
    "            \n",
    "#             # ===== DEM =====\n",
    "#             titles = [glac_str + ' DEM']\n",
    "#             var_full2plot = gf.z1.copy()\n",
    "# #                         var_full2plot.mask = gf.z1.mask\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, titles, 'inferno', 'elev (masl)', \n",
    "#                        close_fig=False)\n",
    "\n",
    "\n",
    "# #             # Plots\n",
    "# #     #         titles = ['Z1']\n",
    "# #     #         z1_full2plot = gf.z1\n",
    "# #     #         z1_full2plot.mask = dems_mask\n",
    "# #     #         clim = malib.calcperc(z1_full2plot, (2,98))\n",
    "# #     #         plot_array(z1_full2plot, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=input.output_fig_fp + glac_str + '_dem.png')\n",
    "\n",
    "# #             #Now apply glacier mask AND mask NaN values\n",
    "# #             glac_geom_mask = np.ma.mask_or(glac_geom_mask, dems_mask)\n",
    "# #             # nan_mask = np.ma.masked_invalid(gf.dz)\n",
    "# #             # glac_geom_mask = np.ma.mask_or(glac_geom_mask, nan_mask.mask)\n",
    "# #             gf.z1 = np.ma.array(gf.z1, mask=glac_geom_mask)\n",
    "            \n",
    "# # #             # Debris cover mask\n",
    "# # #             dc_mask = np.ma.mask_or(dc_shp_lyr_mask, glac_geom_mask)\n",
    "\n",
    "# #             gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "# #             titles = ['Z1 (masked)']\n",
    "# #             clim = malib.calcperc(gf.z1, (2,98))\n",
    "# #             plot_array(gf.z1, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=input.output_fig_fp + glac_str + '_dem.png')\n",
    "\n",
    "# #             if verbose:\n",
    "# #                 print(gf.z1.shape)\n",
    "                \n",
    "# # #             titles = ['Vx']\n",
    "# # #             var_full2plot = gf.vx\n",
    "# # #             var_full2plot.mask = glac_geom_mask\n",
    "# # #             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "# # #             plot_array(var_full2plot, clim, titles, 'inferno', 'vx', fn=input.output_fig_fp + gf.feat_fn +'_vx.png')\n",
    "\n",
    "# # #             titles = ['Vy']\n",
    "# # #             var_full2plot = gf.vy\n",
    "# # #             var_full2plot.mask = glac_geom_mask\n",
    "# # #             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "# # #             plot_array(var_full2plot, clim, titles, 'inferno', 'vy', fn=input.output_fig_fp + gf.feat_fn +'_vy.png')\n",
    "\n",
    "# #             gf.vtot = (gf.vx**2 + gf.vy**2)**0.5\n",
    "\n",
    "# #             titles = ['Velocity (m/yr)']\n",
    "# #             var_full2plot = gf.vtot\n",
    "# #             var_full2plot.mask = glac_geom_mask\n",
    "# #             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "# #             plot_array(var_full2plot, clim, titles, 'inferno', 'Velocity (m/yr)', fn=input.output_fig_fp + glac_str +'_velocity.png',\n",
    "# #                        close_fig=close_fig)\n",
    "\n",
    "# #             titles = ['Ice thickness']\n",
    "# #             var_full2plot = gf.H\n",
    "# #             var_full2plot.mask = glac_geom_mask\n",
    "# # #             var_full2plot.mask = dc_mask\n",
    "# #             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "# #             plot_array(var_full2plot, clim, titles, 'inferno', 'H', fn=input.output_fig_fp + gf.feat_fn +'_ice_thickness.png',\n",
    "# #                       close_fig=close_fig)\n",
    "        \n",
    "            \n",
    "# #             titles = ['Debris cover']\n",
    "# #             var_full2plot = gf.dc_area\n",
    "# #             clim = (0,1)\n",
    "# #             plot_array(var_full2plot, clim, titles, 'inferno', '', fn=input.output_fig_fp + gf.feat_fn +'_debriscover.png',\n",
    "# #                       close_fig=close_fig)\n",
    "            \n",
    "# #             titles = ['Flux']\n",
    "# #             divQ_full2plot = gf.divQ\n",
    "# #             divQ_full2plot.mask = glac_geom_mask\n",
    "# #             clim = malib.calcperc(divQ_full2plot, (2,98))\n",
    "# #             plot_array(divQ_full2plot, clim, titles, 'inferno', 'divQ', fn=input.output_fig_fp + glac_str +'_divQ.png')\n",
    "\n",
    "#             # ===== \"COREGISTER\" SURFACE LOWERING WITH DEM USED FOR ICE THICKNESS =====\n",
    "#             # Load Mass Balance Data and find displacement =====\n",
    "#             if verbose:\n",
    "#                 print('\\nREALLY THIS SHOULD BE DONE BY COREGISTRATION OF THE TWO DEMS\\n')\n",
    "#                 print(main_glac_rgi.loc[glac_idx, 'mb_fn'])\n",
    "#             mb_df = pd.read_csv(main_glac_rgi.loc[glac_idx, 'mb_fn'])\n",
    "#             mb_df.loc[:,:] = mb_df.values.astype(np.float64)\n",
    "            \n",
    "#             print(mb_df)\n",
    "            \n",
    "#             try:\n",
    "#                 mb_bin0_km2 = mb_df.loc[0,' z1_bin_area_perc'] / 100 * main_glac_rgi.loc[glac_idx,'Area']\n",
    "#                 print('here:', main_glac_rgi.loc[glac_idx,'Area'])\n",
    "#             except:\n",
    "#                 mb_bin0_km2 = mb_df.loc[0,' z1_bin_area_valid_km2']\n",
    "                \n",
    "#             print('mb_bin0_km2:', mb_bin0_km2)\n",
    "                \n",
    "#             mb_bin_size = mb_df.loc[1,'# bin_center_elev_m'] - mb_df.loc[0,'# bin_center_elev_m']\n",
    "#             pix_km2 = gf.res[0] * gf.res[1] / (1000)**2\n",
    "#             if verbose:\n",
    "#                 print('total glacier area [km2]:', main_glac_rgi.loc[glac_idx,'Area'])\n",
    "#                 print('initial bin area [km2]:', mb_bin0_km2)\n",
    "#                 print('bin size [m]:', mb_bin_size)\n",
    "#                 print('pixel size [km2]:', pix_km2)\n",
    "#             # Find displacement\n",
    "#             if len(gf.z1.compressed()) > 0:\n",
    "#                 z1 = gf.z1.compressed()\n",
    "#                 z1_min = z1[z1>0].min()\n",
    "#                 z1_max = z1[z1>0].max()\n",
    "#                 z1_km2 = 0\n",
    "#                 elev = int(z1_min)\n",
    "                \n",
    "#                 print('z1_min:', z1_min)\n",
    "                \n",
    "#                 while z1_km2 < mb_bin0_km2 and elev < z1_max:\n",
    "#                     elev += 1\n",
    "#                     z1_idx = np.where((z1 > 0) & (z1 < elev))\n",
    "#                     if len(z1_idx[0]) > 0:\n",
    "#                         z1_km2 = len(z1_idx[0]) * pix_km2\n",
    "#                         print(elev, z1_km2)        \n",
    "#                 if verbose:\n",
    "#                     print(elev, z1_km2, 'vs', mb_df.loc[0,'# bin_center_elev_m'], mb_bin0_km2)\n",
    "#                 mb_bin0_upper =  mb_df.loc[0,'# bin_center_elev_m'] + mb_bin_size / 2\n",
    "#                 z1_offset = elev - mb_bin0_upper\n",
    "#                 if verbose:\n",
    "#                     print('z1_offset:', z1_offset)\n",
    "#                 # Update z1 with the offset\n",
    "#                 mask_offset = np.ma.array(np.zeros(gf.z1.mask.shape) - z1_offset, mask=np.ma.getmask(gf.z1))\n",
    "#                 gf.z1[~gf.z1.mask] = gf.z1[~gf.z1.mask] + mask_offset[~mask_offset.mask]\n",
    "                \n",
    "#                 mask_offset_dc = np.ma.array(np.zeros(gf.dc_area.mask.shape) - z1_offset, mask=np.ma.getmask(gf.dc_area))\n",
    "#                 gf.dc_area[~gf.dc_area.mask] = gf.dc_area[~gf.dc_area.mask] + mask_offset[~mask_offset_dc.mask]\n",
    "\n",
    "# # #                 # ===== EMERGENCE VELOCITY =====\n",
    "# # #                 vx = np.ma.filled(gf.vx,0)\n",
    "# # #                 vy = np.ma.filled(gf.vy,0)\n",
    "# # #                 H = np.ma.filled(gf.H,0)\n",
    "# # #                 vx[gf.z1 > gf.z1.max()] = 0\n",
    "# # #                 vy[gf.z1 > gf.z1.max()] = 0\n",
    "# # #                 H[gf.z1 > gf.z1.max()] = 0\n",
    "# # #                 vmax = np.nanmax((vx**2 + vy**2)**0.5)\n",
    "\n",
    "# # #                 # Emergence computation\n",
    "# # #                 emvel = emergence_pixels(gf, vx, vy, H, gf.res[0], gf.res[1], \n",
    "# # #                                          positive_is_east=True, positive_is_north=True, \n",
    "# # #                                          constant_icethickness=False, max_velocity=vmax, vel_min=0, debug=False)\n",
    "# # #                 # 3x3 filter to reduce\n",
    "# # #                 if input.emvel_filter_pixsize > 0:\n",
    "# # #                     emvel = ndimage.filters.convolve(emvel, weights=np.full((input.emvel_filter_pixsize, input.emvel_filter_pixsize), \n",
    "# # #                                                                             1.0/input.emvel_filter_pixsize**2))\n",
    "# # #                 # Add to glacier feature\n",
    "# # #                 gf.emvel = np.ma.masked_array(emvel, mask=np.ma.getmask(gf.z1))\n",
    "\n",
    "# # #                 # ===== EXPORT BINNED STATISTICS =====\n",
    "# # #                 #Do AED for all\n",
    "# # #                 #Compute mb using scaled AED vs. polygon\n",
    "# # #                 #Check for valid pixel count vs. feature area, fill if appropriate\n",
    "# # #                 if gf.glac_area_km2 > input.min_glac_area_writeout:\n",
    "# # #                     outbins_df, z_bin_edges = hist_plot(gf, bin_width=mb_bin_size, csv_ending=csv_ending,\n",
    "# # #                                                         mb_df=mb_df)\n",
    "\n",
    "# # # #                     if verbose:\n",
    "# # # #                         print(outbins_df.loc[0:10,['bin_center_elev_m', ' vm_med',' vm_mad', ' H_mean', ' H_std', \n",
    "# # # #                                                    ' emvel_mean', ' emvel_std',' emvel_med', ' emvel_mad']])\n",
    "# # #             else:\n",
    "# # #                 print('\\n' + glac_str + ' HAS NO GLACIER AREA!\\n')\n",
    "# mb_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
