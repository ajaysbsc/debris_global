{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot_array(dem, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None):\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    #Gray background\n",
    "    ax.set_facecolor('0.5')\n",
    "    #Force aspect ratio to match images\n",
    "    ax.set(aspect='equal')\n",
    "    #Turn off axes labels/ticks\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if titles is not None:\n",
    "        ax.set_title(titles[0])\n",
    "    #Plot background shaded relief map\n",
    "    if overlay is not None:\n",
    "        alpha = 0.7\n",
    "        ax.imshow(overlay, cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [ax.imshow(dem, clim=clim, cmap=cmap, alpha=alpha)]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "    plt.close(fig)\n",
    "        \n",
    "def selectglaciersrgitable(glac_no=None,\n",
    "                           rgi_regionsO1=None,\n",
    "                           rgi_regionsO2=None,\n",
    "                           rgi_glac_number=None,\n",
    "#                            rgi_fp=input.rgi_fp,\n",
    "                           rgi_fp = '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/RGI/rgi60/00_rgi60_attribs/',\n",
    "                           rgi_cols_drop=['GLIMSId','BgnDate','EndDate','Status','Connect','Linkages','Name'],\n",
    "                           rgi_O1Id_colname='glacno',\n",
    "                           rgi_glacno_float_colname='RGIId_float',\n",
    "                           indexname='GlacNo'):\n",
    "    \"\"\"\n",
    "    Select all glaciers to be used in the model run according to the regions and glacier numbers defined by the RGI\n",
    "    glacier inventory. This function returns the rgi table associated with all of these glaciers.\n",
    "\n",
    "    glac_no : list of strings\n",
    "        list of strings of RGI glacier numbers (e.g., ['1.00001', '13.00001'])\n",
    "    rgi_regionsO1 : list of integers\n",
    "        list of integers of RGI order 1 regions (e.g., [1, 13])\n",
    "    rgi_regionsO2 : list of integers or 'all'\n",
    "        list of integers of RGI order 2 regions or simply 'all' for all the order 2 regions\n",
    "    rgi_glac_number : list of strings\n",
    "        list of RGI glacier numbers without the region (e.g., ['00001', '00002'])\n",
    "\n",
    "    Output: Pandas DataFrame of the glacier statistics for each glacier in the model run\n",
    "    (rows = GlacNo, columns = glacier statistics)\n",
    "    \"\"\"\n",
    "    if glac_no is not None:\n",
    "        glac_no_byregion = {}\n",
    "        rgi_regionsO1 = [int(i.split('.')[0]) for i in glac_no]\n",
    "        rgi_regionsO1 = list(set(rgi_regionsO1))\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = []\n",
    "        for i in glac_no:\n",
    "            region = i.split('.')[0]\n",
    "            glac_no_only = i.split('.')[1]\n",
    "            glac_no_byregion[int(region)].append(glac_no_only)\n",
    "\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = sorted(glac_no_byregion[region])\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    rgi_regionsO1 = sorted(rgi_regionsO1)\n",
    "    glacier_table = pd.DataFrame()\n",
    "    for region in rgi_regionsO1:\n",
    "\n",
    "        if glac_no is not None:\n",
    "            rgi_glac_number = glac_no_byregion[region]\n",
    "\n",
    "#        if len(rgi_glac_number) < 50:\n",
    "\n",
    "        for i in os.listdir(rgi_fp):\n",
    "            if i.startswith(str(region).zfill(2)) and i.endswith('.csv'):\n",
    "                rgi_fn = i\n",
    "        try:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp + rgi_fn)\n",
    "        except:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp + rgi_fn, encoding='latin1')\n",
    "        \n",
    "        # Populate glacer_table with the glaciers of interest\n",
    "        if rgi_regionsO2 == 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within region(s) %s are included in this model run.\" % (region))\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1\n",
    "            else:\n",
    "                glacier_table = pd.concat([glacier_table, csv_regionO1], axis=0)\n",
    "        elif rgi_regionsO2 != 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within subregion(s) %s in region %s are included in this model run.\" %\n",
    "                  (rgi_regionsO2, region))\n",
    "            for regionO2 in rgi_regionsO2:\n",
    "                if glacier_table.empty:\n",
    "                    glacier_table = csv_regionO1.loc[csv_regionO1['O2Region'] == regionO2]\n",
    "                else:\n",
    "                    glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[csv_regionO1['O2Region'] ==\n",
    "                                                                                regionO2]], axis=0))\n",
    "        else:\n",
    "            if len(rgi_glac_number) < 20:\n",
    "                print(\"%s glaciers in region %s are included in this model run: %s\" % (len(rgi_glac_number), region,\n",
    "                                                                                       rgi_glac_number))\n",
    "            else:\n",
    "                print(\"%s glaciers in region %s are included in this model run: %s and more\" %\n",
    "                      (len(rgi_glac_number), region, rgi_glac_number[0:50]))\n",
    "                \n",
    "            rgiid_subset = ['RGI60-' + str(region).zfill(2) + '.' + x for x in rgi_glac_number] \n",
    "            rgiid_all = list(csv_regionO1.RGIId.values)\n",
    "            rgi_idx = [rgiid_all.index(x) for x in rgiid_subset]\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1.loc[rgi_idx]\n",
    "            else:\n",
    "                glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[rgi_idx]],\n",
    "                                           axis=0))\n",
    "                    \n",
    "    glacier_table = glacier_table.copy()\n",
    "    # reset the index so that it is in sequential order (0, 1, 2, etc.)\n",
    "    glacier_table.reset_index(inplace=True)\n",
    "    # change old index to 'O1Index' to be easier to recall what it is\n",
    "    glacier_table.rename(columns={'index': 'O1Index'}, inplace=True)\n",
    "    # Record the reference date\n",
    "    glacier_table['RefDate'] = glacier_table['BgnDate']\n",
    "    # if there is an end date, then roughly average the year\n",
    "    enddate_idx = glacier_table.loc[(glacier_table['EndDate'] > 0), 'EndDate'].index.values\n",
    "    glacier_table.loc[enddate_idx,'RefDate'] = (\n",
    "            np.mean((glacier_table.loc[enddate_idx,['BgnDate', 'EndDate']].values / 10**4).astype(int),\n",
    "                    axis=1).astype(int) * 10**4 + 9999)\n",
    "    # drop columns of data that is not being used\n",
    "    glacier_table.drop(rgi_cols_drop, axis=1, inplace=True)\n",
    "    # add column with the O1 glacier numbers\n",
    "    glacier_table[rgi_O1Id_colname] = (\n",
    "            glacier_table['RGIId'].str.split('.').apply(pd.Series).loc[:,1].astype(int))\n",
    "    glacier_table['rgino_str'] = [x.split('-')[1] for x in glacier_table.RGIId.values]\n",
    "    glacier_table[rgi_glacno_float_colname] = (np.array([np.str.split(glacier_table['RGIId'][x],'-')[1]\n",
    "                                                    for x in range(glacier_table.shape[0])]).astype(float))\n",
    "    # set index name\n",
    "    glacier_table.index.name = indexname\n",
    "\n",
    "    print(\"This study is focusing on %s glaciers in region %s\" % (glacier_table.shape[0], rgi_regionsO1))\n",
    "\n",
    "    return glacier_table\n",
    "\n",
    "\n",
    "def nearest_nonzero_idx(a,x,y):\n",
    "    r,c = np.nonzero(a)\n",
    "    min_idx = ((r - x)**2 + (c - y)**2).argmin()\n",
    "    return r[min_idx], c[min_idx]\n",
    "\n",
    "\n",
    "def emergence_pixels(gf, vel_x_raw, vel_y_raw, icethickness_raw, xres, yres, \n",
    "                     vel_min=0, max_velocity=600, vel_depth_avg_factor=0.8, option_border=1,\n",
    "                     positive_is_east=True, positive_is_north=True, constant_icethickness=False, debug=True):\n",
    "    \"\"\" Compute the emergence velocity using an ice flux approach\n",
    "    \"\"\"\n",
    "    # Glacier mask\n",
    "    glac_mask = np.zeros(vel_x_raw.shape) + 1\n",
    "    glac_mask[gf.z1.mask] = 0\n",
    "    \n",
    "    # Modify vel_y by multiplying velocity by -1 such that matrix operations agree with flow direction\n",
    "    #    Specifically, a negative y velocity means the pixel is flowing south.\n",
    "    #    However, if you were to subtract that value from the rows, it would head north in the matrix.\n",
    "    #    This is due to the fact that the number of rows start at 0 at the top.\n",
    "    #    Therefore, multipylying by -1 aligns the matrix operations with the flow direction\n",
    "    if positive_is_north:\n",
    "        vel_y = -1*vel_y_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_y = vel_y_raw * vel_depth_avg_factor\n",
    "    if positive_is_east:\n",
    "        vel_x = vel_x_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_x = -1*vel_x_raw * vel_depth_avg_factor\n",
    "    vel_total = (vel_y**2 + vel_x**2)**0.5\n",
    "    # Ice thickness\n",
    "    icethickness = icethickness_raw.copy()\n",
    "    if constant_icethickness:\n",
    "        icethickness[:,:] = 1\n",
    "        icethickness = icethickness * glac_mask\n",
    "#     print('mean ice thickness:', np.round(icethickness.mean(),0), 'm')\n",
    "    # Compute the initial volume\n",
    "    volume_initial = icethickness * (xres * yres)\n",
    "    pix_maxres = xres\n",
    "    if yres > pix_maxres:\n",
    "        pix_maxres = yres\n",
    "    # Quality control options:\n",
    "    # Apply a border based on the max specified velocity to prevent errors associated with pixels going out of bounds\n",
    "    if option_border == 1:\n",
    "        border = int(max_velocity / pix_maxres) + 1\n",
    "        for r in range(vel_x.shape[0]):\n",
    "            for c in range(vel_x.shape[1]):\n",
    "                if (r < border) | (r >= vel_x.shape[0] - border) | (c < border) | (c >= vel_x.shape[1] - border):\n",
    "                    vel_x[r,c] = 0\n",
    "                    vel_y[r,c] = 0\n",
    "    # Minimum/maximum velocity bounds\n",
    "    vel_x[vel_total < vel_min] = 0\n",
    "    vel_y[vel_total < vel_min] = 0\n",
    "    vel_x[vel_total > max_velocity] = 0\n",
    "    vel_y[vel_total > max_velocity] = 0\n",
    "#     # Remove clusters of high velocity on stagnant portions of glaciers due to feature tracking of ice cliffs and ponds\n",
    "#     if option_stagnantbands == 1:\n",
    "#         vel_x[bands <= stagnant_band] = 0\n",
    "#         vel_y[bands <= stagnant_band] = 0        \n",
    "    # Compute displacement in units of pixels\n",
    "    vel_x_pix = vel_x / xres\n",
    "    vel_y_pix = vel_y / yres\n",
    "    # Compute the displacement and fraction of pixels moved for all columns (x-axis)\n",
    "    # col_x1 is the number of columns to the closest pixel receiving ice [ex. 2.6 returns 2, -2.6 returns -2]\n",
    "    #    int() automatically rounds towards zero\n",
    "    col_x1 = vel_x_pix.astype(int)\n",
    "    # col_x2 is the number of columns to the further pixel receiving ice [ex. 2.6 returns 3, -2.6 returns -3]\n",
    "    #    np.sign() returns a value of 1 or -1, so it's adding 1 pixel away from zero\n",
    "    col_x2 = (vel_x_pix + np.sign(vel_x_pix)).astype(int)\n",
    "    # rem_x2 is the fraction of the pixel that remains in the further pixel (col_x2) [ex. 2.6 returns 0.6, -2.6 returns 0.6]\n",
    "    #    np.sign() returns a value of 1 or -1, so multiplying by that ensures you have a positive value\n",
    "    #    then when you take the remainder using \"% 1\", you obtain the desired fraction\n",
    "    rem_x2 = np.multiply(np.sign(vel_x_pix), vel_x_pix) % 1\n",
    "    # rem_x1 is the fraction of the pixel that remains in the closer pixel (col_x1) [ex. 2.6 returns 0.4, -2.6 returns 0.4]\n",
    "    rem_x1 = 1 - rem_x2\n",
    "    # Repeat the displacement and fraction computations for all rows (y-axis)\n",
    "    row_y1 = vel_y_pix.astype(int)\n",
    "    row_y2 = (vel_y_pix + np.sign(vel_y_pix)).astype(int)\n",
    "    rem_y2 = np.multiply(np.sign(vel_y_pix), vel_y_pix) % 1\n",
    "    rem_y1 = 1 - rem_y2\n",
    "          \n",
    "    # Compute the mass flux for each pixel\n",
    "    volume_final = np.zeros(volume_initial.shape)\n",
    "    for r in range(vel_x.shape[0]):\n",
    "        for c in range(vel_x.shape[1]):\n",
    "            volume_final[r+row_y1[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x1[r,c]] + rem_y1[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x1[r,c]] + rem_y2[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y1[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x2[r,c]] + rem_y1[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x2[r,c]] + rem_y2[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "         \n",
    "    # Redistribute off-glacier volume back onto the nearest pixel on the glacier\n",
    "    offglac_row, offglac_col = np.where((glac_mask == 0) & (volume_final > 0))\n",
    "    for nidx in range(0,len(offglac_row)):\n",
    "        nrow = offglac_row[nidx]\n",
    "        ncol = offglac_col[nidx]\n",
    "        ridx, cidx = nearest_nonzero_idx(glac_mask, nrow, ncol)\n",
    "        # Add off-glacier volume back onto nearest pixel on glacier\n",
    "        volume_final[ridx,cidx] += volume_final[nrow,ncol]\n",
    "        volume_final[nrow,ncol] = 0\n",
    "            \n",
    "    # Check that mass is conserved (threshold = 0.1 m x pixel_size**2)\n",
    "    if debug:\n",
    "        print('Mass is conserved?', np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() < 0.01)\n",
    "        print(np.round(np.absolute(volume_final.sum() - volume_initial.sum()),1), \n",
    "              np.round(np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() * 100,2), '%')\n",
    "        \n",
    "    if np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() > 0.01:\n",
    "        print('MASS NOT CONSERVED FOR EMERGENCE VELOCITY')\n",
    "    # Final ice thickness\n",
    "    icethickness_final = volume_final / (xres * yres)\n",
    "    # Emergence velocity\n",
    "    emergence_velocity = icethickness_final - icethickness\n",
    "    return emergence_velocity\n",
    "\n",
    "\n",
    "\n",
    "class GlacFeat:\n",
    "    def __init__(self, feat, glacname_fieldname, glacnum_fieldname):\n",
    "\n",
    "        self.glacname = feat.GetField(glacname_fieldname)\n",
    "        if self.glacname is None:\n",
    "            self.glacname = \"\"\n",
    "        else:\n",
    "            #RGI has some nonstandard characters\n",
    "            #self.glacname = self.glacname.decode('unicode_escape').encode('ascii','ignore')\n",
    "            #glacname = re.sub(r'[^\\x00-\\x7f]',r'', glacname)\n",
    "            self.glacname = re.sub(r'\\W+', '', self.glacname)\n",
    "            self.glacname = self.glacname.replace(\" \", \"\")\n",
    "            self.glacname = self.glacname.replace(\"_\", \"\")\n",
    "            self.glacname = self.glacname.replace(\"/\", \"\")\n",
    "\n",
    "        self.glacnum = feat.GetField(glacnum_fieldname)\n",
    "        fn = feat.GetDefnRef().GetName()\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        self.glacnum = '%0.5f' % float(self.glacnum.split('-')[-1])\n",
    "\n",
    "        if self.glacname:\n",
    "            self.feat_fn = \"%s_%s\" % (self.glacnum, self.glacname)\n",
    "        else:\n",
    "            self.feat_fn = str(self.glacnum)\n",
    "\n",
    "        self.glac_geom_orig = geolib.geom_dup(feat.GetGeometryRef())\n",
    "        self.glac_geom = geolib.geom_dup(self.glac_geom_orig)\n",
    "        #Hack to deal with fact that this is not preserved in geom when loaded from pickle on disk\n",
    "        self.glac_geom_srs_wkt = self.glac_geom.GetSpatialReference().ExportToWkt()\n",
    "\n",
    "        #Attributes written by mb_calc\n",
    "        self.z1 = None\n",
    "        self.z1_hs = None\n",
    "        self.z1_stats = None\n",
    "        self.z1_ela = None\n",
    "        self.z2 = None\n",
    "        self.z2_hs = None\n",
    "        self.z2_stats = None\n",
    "        self.z2_ela = None\n",
    "        self.z2_aspect = None\n",
    "        self.z2_aspect_stats = None\n",
    "        self.z2_slope = None\n",
    "        self.z2_slope_stats = None\n",
    "        self.res = None\n",
    "        self.dhdt = None\n",
    "        self.mb = None\n",
    "        self.mb_mean = None\n",
    "        self.t1 = None\n",
    "        self.t2 = None\n",
    "        self.dt = None\n",
    "        self.t1_mean = None\n",
    "        self.t2_mean = None\n",
    "        self.dt_mean = None\n",
    "\n",
    "        self.H = None\n",
    "        self.H_mean = np.nan\n",
    "        self.vx = None\n",
    "        self.vy = None\n",
    "        self.vm = None\n",
    "        self.vm_mean = np.nan\n",
    "        self.divQ = None\n",
    "        self.emvel = None\n",
    "        self.debris_class = None\n",
    "        self.debris_thick = None\n",
    "        self.debris_thick_mean = np.nan\n",
    "        self.perc_clean = np.nan\n",
    "        self.perc_debris = np.nan\n",
    "        self.perc_pond = np.nan\n",
    "\n",
    "    def geom_srs_update(self, srs=None):\n",
    "        if self.glac_geom.GetSpatialReference() is None:\n",
    "            if srs is None:\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromWkt(self.glac_geom_srs_wkt)\n",
    "            self.glac_geom.AssignSpatialReference(srs)\n",
    "\n",
    "    def geom_attributes(self, srs=None):\n",
    "        self.geom_srs_update()\n",
    "        if srs is not None:\n",
    "            #Should reproject here to equal area, before geom_attributes\n",
    "            #self.glac_geom.AssignSpatialReference(glac_shp_srs)\n",
    "            #self.glac_geom_local = geolib.geom2localortho(self.glac_geom)\n",
    "            geolib.geom_transform(self.glac_geom, srs)\n",
    "\n",
    "        self.glac_geom_extent = geolib.geom_extent(self.glac_geom)\n",
    "        self.glac_area = self.glac_geom.GetArea()\n",
    "        self.glac_area_km2 = self.glac_area / 1E6\n",
    "        self.cx, self.cy = self.glac_geom.Centroid().GetPoint_2D()\n",
    "        \n",
    "        \n",
    "#RGI uses 50 m bins\n",
    "def hist_plot(gf, bin_width=50.0, dz_clim=(-2.0, 2.0), exportcsv=True, csv_ending=''):\n",
    "    #print(\"Generating histograms\")\n",
    "    #Create bins for full range of input data and specified bin width\n",
    "\n",
    "    #NOTE: these counts/areas are for valid pixels only\n",
    "    #Not necessarily a true representation of actual glacier hypsometry\n",
    "    #Need a void-filled DEM for this\n",
    "\n",
    "    z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "    #Need to compress here, otherwise histogram uses masked values!\n",
    "    z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "    z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "    #RGI standard is integer thousandths of glaciers total area\n",
    "    #Should check to make sure sum of bin areas equals total area\n",
    "    #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "    z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #If we only have one elevation grid with dhdt\n",
    "    if gf.z2 is not None:\n",
    "        z2_bin_counts, z2_bin_edges = np.histogram(gf.z2.compressed(), bins=z_bin_edges)\n",
    "        z2_bin_areas = z2_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #z2_bin_areas_perc = 100. * z2_bin_areas / np.sum(z2_bin_areas)\n",
    "        z2_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "    else:\n",
    "        z2_bin_counts = z1_bin_counts\n",
    "        z2_bin_edges = z1_bin_edges\n",
    "        z2_bin_areas = z1_bin_areas\n",
    "        z2_bin_areas_perc = z1_bin_areas_perc\n",
    "\n",
    "    #Create arrays to store output\n",
    "    slope_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    slope_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.dhdt is not None:\n",
    "        mb_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        np.ma.set_fill_value(mb_bin_med, np.nan)\n",
    "        mb_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_count = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.vm is not None:\n",
    "        vm_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        vm_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.H is not None:\n",
    "        H_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        H_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.emvel is not None:\n",
    "        emvel_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.debris_class is not None:\n",
    "#         perc_clean = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_debris = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_pond = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_clean_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_debris_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_pond_bin_med = np.ma.masked_all_like(mz1_bin_areas)\n",
    "\n",
    "#         gf.dhdt_clean = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 1).data))\n",
    "#         gf.dhdt_debris = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 2).data))\n",
    "#         gf.dhdt_pond = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 3).data))\n",
    "\n",
    "    if gf.debris_thick_ts is not None:\n",
    "        debris_thick_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        meltfactor_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        meltfactor_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "\n",
    "    #Bin sample count must be greater than this value\n",
    "    min_bin_samp_count = 9\n",
    "\n",
    "    #Loop through each bin and extract stats\n",
    "    idx = np.digitize(gf.z1, z_bin_edges)\n",
    "    for bin_n in range(z_bin_centers.size):\n",
    "        if gf.dhdt is not None:\n",
    "            mb_bin_samp = gf.mb_map[(idx == bin_n+1)]\n",
    "            if mb_bin_samp.count() > min_bin_samp_count:\n",
    "                mb_bin_med[bin_n] = malib.fast_median(mb_bin_samp)\n",
    "                mb_bin_mad[bin_n] = malib.mad(mb_bin_samp)\n",
    "                mb_bin_mean[bin_n] = mb_bin_samp.mean()\n",
    "                mb_bin_std[bin_n] = mb_bin_samp.std()\n",
    "            dhdt_bin_samp = gf.dhdt[(idx == bin_n+1)]\n",
    "            if dhdt_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_bin_med[bin_n] = malib.fast_median(dhdt_bin_samp)\n",
    "                dhdt_bin_mad[bin_n] = malib.mad(dhdt_bin_samp)\n",
    "                dhdt_bin_mean[bin_n] = dhdt_bin_samp.mean()\n",
    "                dhdt_bin_std[bin_n] = dhdt_bin_samp.std()\n",
    "                dhdt_bin_count[bin_n] = dhdt_bin_samp.count()\n",
    "        if gf.debris_thick is not None:\n",
    "            debris_thick_bin_samp = gf.debris_thick[(idx == bin_n+1)]\n",
    "            if debris_thick_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_med[bin_n] = malib.fast_median(debris_thick_bin_samp)\n",
    "                debris_thick_mad[bin_n] = malib.mad(debris_thick_bin_samp)\n",
    "        \n",
    "        if gf.debris_thick_ts is not None:\n",
    "            debris_thick_ts_bin_samp = gf.debris_thick_ts[(idx == bin_n+1)]\n",
    "            if debris_thick_ts_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_ts_med[bin_n] = malib.fast_median(debris_thick_ts_bin_samp)\n",
    "                debris_thick_ts_mad[bin_n] = malib.mad(debris_thick_ts_bin_samp)\n",
    "        if gf.meltfactor_ts is not None:\n",
    "            meltfactor_ts_bin_samp = gf.meltfactor_ts[(idx == bin_n+1)]\n",
    "            if meltfactor_ts_bin_samp.size > min_bin_samp_count:\n",
    "                meltfactor_ts_med[bin_n] = malib.fast_median(meltfactor_ts_bin_samp)\n",
    "                meltfactor_ts_mad[bin_n] = malib.mad(meltfactor_ts_bin_samp)\n",
    "                \n",
    "        if gf.debris_class is not None:\n",
    "            debris_class_bin_samp = gf.debris_class[(idx == bin_n+1)]\n",
    "            dhdt_clean_bin_samp = gf.dhdt_clean[(idx == bin_n+1)]\n",
    "            dhdt_debris_bin_samp = gf.dhdt_debris[(idx == bin_n+1)]\n",
    "            dhdt_pond_bin_samp = gf.dhdt_pond[(idx == bin_n+1)]\n",
    "            if debris_class_bin_samp.count() > min_bin_samp_count:\n",
    "                perc_clean[bin_n] = 100. * (debris_class_bin_samp == 1).sum()/debris_class_bin_samp.count()\n",
    "                perc_debris[bin_n] = 100. * (debris_class_bin_samp == 2).sum()/debris_class_bin_samp.count()\n",
    "                perc_pond[bin_n] = 100. * (debris_class_bin_samp == 3).sum()/debris_class_bin_samp.count()\n",
    "            if dhdt_clean_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_clean_bin_med[bin_n] = malib.fast_median(dhdt_clean_bin_samp)\n",
    "            if dhdt_debris_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_debris_bin_med[bin_n] = malib.fast_median(dhdt_debris_bin_samp)\n",
    "            if dhdt_pond_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_pond_bin_med[bin_n] = malib.fast_median(dhdt_pond_bin_samp)\n",
    "        if gf.vm is not None:\n",
    "            vm_bin_samp = gf.vm[(idx == bin_n+1)]\n",
    "            if vm_bin_samp.size > min_bin_samp_count:\n",
    "                vm_bin_med[bin_n] = malib.fast_median(vm_bin_samp)\n",
    "                vm_bin_mad[bin_n] = malib.mad(vm_bin_samp)\n",
    "        if gf.H is not None:\n",
    "            H_bin_samp = gf.H[(idx == bin_n+1)]\n",
    "            if H_bin_samp.size > min_bin_samp_count:\n",
    "                H_bin_mean[bin_n] = H_bin_samp.mean()\n",
    "                H_bin_std[bin_n] = H_bin_samp.std()\n",
    "        if gf.emvel is not None:\n",
    "            emvel_bin_samp = gf.emvel[(idx == bin_n+1)]\n",
    "            if emvel_bin_samp.size > min_bin_samp_count:\n",
    "                emvel_bin_mean[bin_n] = emvel_bin_samp.mean()\n",
    "                emvel_bin_std[bin_n] = emvel_bin_samp.std()\n",
    "                emvel_bin_med[bin_n] = malib.fast_median(emvel_bin_samp)\n",
    "                emvel_bin_mad[bin_n] = malib.mad(emvel_bin_samp)\n",
    "        slope_bin_samp = gf.z1_slope[(idx == bin_n+1)]\n",
    "        if slope_bin_samp.size > min_bin_samp_count:\n",
    "            slope_bin_med[bin_n] = malib.fast_median(slope_bin_samp)\n",
    "            slope_bin_mad[bin_n] = malib.mad(slope_bin_samp)\n",
    "        aspect_bin_samp = gf.z1_aspect[(idx == bin_n+1)]\n",
    "        if aspect_bin_samp.size > min_bin_samp_count:\n",
    "            aspect_bin_med[bin_n] = malib.fast_median(aspect_bin_samp)\n",
    "            aspect_bin_mad[bin_n] = malib.mad(aspect_bin_samp)\n",
    "\n",
    "    if gf.dhdt is not None:\n",
    "        dhdt_bin_areas = dhdt_bin_count * gf.res[0] * gf.res[1] / 1E6\n",
    "        #dhdt_bin_areas_perc = 100. * dhdt_bin_areas / np.sum(dhdt_bin_areas)\n",
    "        dhdt_bin_areas_perc = 100. * (dhdt_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    outbins_header = 'bin_center_elev_m, z1_bin_count_valid, z1_bin_area_valid_km2, z1_bin_area_perc, z2_bin_count_valid, z2_bin_area_valid_km2, z2_bin_area_perc, slope_bin_med, aspect_bin_med'\n",
    "    fmt = '%0.1f, %0.0f, %0.3f, %0.2f, %0.0f, %0.3f, %0.2f, %0.2f, %0.2f'\n",
    "    outbins = [z_bin_centers, z1_bin_counts, z1_bin_areas, z1_bin_areas_perc, z2_bin_counts, z2_bin_areas, z2_bin_areas_perc, slope_bin_med, aspect_bin_med]\n",
    "    if gf.dhdt is not None:\n",
    "        outbins_header += ', dhdt_bin_count, dhdt_bin_area_valid_km2, dhdt_bin_area_perc, dhdt_bin_med_ma, dhdt_bin_mad_ma, dhdt_bin_mean_ma, dhdt_bin_std_ma, mb_bin_med_mwea, mb_bin_mad_mwea, mb_bin_mean_mwea, mb_bin_std_mwea'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([dhdt_bin_count, dhdt_bin_areas, dhdt_bin_areas_perc, dhdt_bin_med, dhdt_bin_mad, dhdt_bin_mean, dhdt_bin_std, \\\n",
    "                        mb_bin_med, mb_bin_mad, mb_bin_mean, mb_bin_std])\n",
    "    if gf.debris_thick is not None:\n",
    "        outbins_header += ', debris_thick_med_m, debris_thick_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_med[debris_thick_med == -(np.inf)] = 0.00\n",
    "        debris_thick_mad[debris_thick_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_med, debris_thick_mad])\n",
    "    \n",
    "    if gf.debris_thick_ts is not None:\n",
    "        outbins_header += ',debris_thick_ts_med_m,debris_thick_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_ts_med[debris_thick_ts_med == -(np.inf)] = 0.00\n",
    "        debris_thick_ts_mad[debris_thick_ts_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_ts_med, debris_thick_ts_mad])\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        outbins_header += ',meltfactor_ts_med_m,meltfactor_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        meltfactor_ts_med[meltfactor_ts_med == -(np.inf)] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med > 1] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med <= 0] = 1\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad == -(np.inf)] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad > 1] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad <= 0] = 0\n",
    "        outbins.extend([meltfactor_ts_med, meltfactor_ts_mad])\n",
    "    \n",
    "    if gf.debris_class is not None:\n",
    "        outbins_header += ', perc_debris, perc_pond, perc_clean, dhdt_debris_med, dhdt_pond_med, dhdt_clean_med'\n",
    "        fmt += ', %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([perc_debris, perc_pond, perc_clean, dhdt_debris_bin_med, dhdt_pond_bin_med, dhdt_clean_bin_med])\n",
    "    if gf.vm is not None:\n",
    "        outbins_header += ', vm_med, vm_mad'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([vm_bin_med, vm_bin_mad])\n",
    "    if gf.H is not None:\n",
    "        outbins_header += ', H_mean, H_std'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([H_bin_mean, H_bin_std])\n",
    "#         outbins_header += ', H_mean, H_std, emvel_mean, emvel_std'\n",
    "#         fmt += ', %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "#         outbins.extend([H_bin_mean, H_bin_std, emvel_bin_mean, emvel_bin_std])\n",
    "\n",
    "    if gf.emvel is not None:\n",
    "        outbins_header += ', emvel_mean, emvel_std, emvel_med, emvel_mad'\n",
    "        fmt += ', %0.3f, %0.3f, %0.3f, %0.3f'\n",
    "        outbins.extend([emvel_bin_mean, emvel_bin_std, emvel_bin_med, emvel_bin_mad])\n",
    "\n",
    "    outbins = np.ma.array(outbins).T.astype('float32')\n",
    "    np.ma.set_fill_value(outbins, np.nan)\n",
    "    outbins = outbins.filled(np.nan)\n",
    "    if exportcsv:\n",
    "        outbins_fn = os.path.join(outdir_csv, gf.feat_fn[0:8] + csv_ending)\n",
    "        np.savetxt(outbins_fn, outbins, fmt=fmt, delimiter=',', header=outbins_header)\n",
    "\n",
    "#     #Create plots of elevation bins\n",
    "#     #print(\"Generating aed plot\")\n",
    "#     #f,axa = plt.subplots(1,2, figsize=(6, 6))\n",
    "#     nsubplots = 0\n",
    "#     if gf.dhdt is not None:\n",
    "#         nsubplots += 1\n",
    "#     if gf.debris_thick is not None:\n",
    "#         nsubplots += 1\n",
    "#     if gf.vm is not None:\n",
    "#         nsubplots += 1\n",
    "#     if gf.H is not None:\n",
    "#         nsubplots += 1\n",
    "# #     print(nsubplots)\n",
    "#     f,axa = plt.subplots(1,nsubplots, squeeze=False, figsize=(10, 7.5))\n",
    "#     f.suptitle(gf.feat_fn)\n",
    "#     fs = 9\n",
    "#     nplot = -1\n",
    "#     if gf.dhdt is not None:\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].plot(z1_bin_areas, z_bin_centers, label='%0.2f' % gf.t1_mean)\n",
    "#         axa[0,nplot].axhline(gf.z1_ela, ls=':', c='C0')\n",
    "#         if gf.z2 is not None:\n",
    "#             axa[0,nplot].plot(z2_bin_areas, z_bin_centers, label='%0.2f' % gf.t2_mean)\n",
    "#             axa[0,nplot].axhline(gf.z2_ela, ls=':', c='C1')\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "#         axa[0,nplot].set_ylabel('Elevation (m WGS84)', fontsize=fs)\n",
    "#         axa[0,nplot].set_xlabel('Area $\\mathregular{km^2}$', fontsize=fs)\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         # pltlib.minorticks_on(axa[0])\n",
    "\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].axvline(0, lw=1.0, c='k')\n",
    "#         \"\"\"\n",
    "#         #Plot flux divergence values for each bin\n",
    "#         if gf.vm is not None and gf.H is not None:\n",
    "#             divQ_bin_mean = np.gradient(H_bin_mean * vm_bin_med * v_col_f)\n",
    "#             axa[1].plot(divQ_bin_mean, z_bin_centers, color='green')\n",
    "#         \"\"\"\n",
    "#         axa[0,nplot].plot(mb_bin_med, z_bin_centers, color='k')\n",
    "#         axa[0,nplot].axvline(gf.mb_mean, lw=0.5, ls=':', c='k', label='%0.2f m w.e./yr' % gf.mb_mean)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, mb_bin_med-mb_bin_mad, mb_bin_med+mb_bin_mad, color='k', alpha=0.1)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, 0, mb_bin_med, where=(mb_bin_med<0), color='r', alpha=0.2)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, 0, mb_bin_med, where=(mb_bin_med>0), color='b', alpha=0.2)\n",
    "#         #axa[nplot].set_xlabel('dh/dt (m/yr)')\n",
    "#         axa[0,nplot].set_xlabel('Mass balance (m w.e./yr)', fontsize=fs)\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         # pltlib.minorticks_on(axa[1])\n",
    "#         #Hide y-axis labels\n",
    "#         axa[0,nplot].axes.yaxis.set_ticklabels([])\n",
    "#         axa[0,nplot].set_xlim(*dz_clim)\n",
    "\n",
    "#     if gf.debris_thick is not None:\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].errorbar(debris_thick_med*100., z_bin_centers, xerr=debris_thick_mad*100, color='k', fmt='o', ms=3, label='Debris Thickness', alpha=0.6)\n",
    "#     if gf.debris_class is not None:\n",
    "#         axa[0,nplot].plot(perc_debris, z_bin_centers, color='sienna', label='Debris Coverage')\n",
    "#         axa[0,nplot].plot(perc_pond, z_bin_centers, color='turquoise', label='Pond Coverage')\n",
    "#     if gf.debris_thick is not None or gf.debris_class is not None:\n",
    "#         axa[0,nplot].set_xlim(0, 100)\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         # pltlib.minorticks_on(axa[2])\n",
    "#         axa[0,nplot].axes.yaxis.set_ticklabels([])\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "#         axa[0,nplot].set_xlabel('Debris thickness (cm), coverage (%)', fontsize=fs)\n",
    "\n",
    "#     if gf.H is not None:\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].plot(H_bin_mean, z_bin_centers, color='b', label='H (%0.2f m)' % gf.H_mean)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, H_bin_mean-H_bin_std, H_bin_mean+H_bin_std, color='b', alpha=0.1)\n",
    "#         axa[0,nplot].set_xlabel('Ice Thickness (m)', fontsize=fs)\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='lower right')\n",
    "#         # pltlib.minorticks_on(axa[3])\n",
    "#         #axa[nplot].set_xlim(0, 400)\n",
    "#         axa[0,nplot].yaxis.tick_left()\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         axa[0,nplot].yaxis.set_label_position(\"right\")\n",
    "    \n",
    "#     if gf.vm is not None:\n",
    "#         nplot += 1\n",
    "# #         ax4 = axa[0,nplot].twinx()\n",
    "#         axa[0,nplot].set_xlabel('Velocity (m/yr)', fontsize=fs)\n",
    "#         axa[0,nplot].plot(vm_bin_med, z_bin_centers, color='g', label='Vm (%0.2f m/yr)' % gf.vm_mean)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, vm_bin_med-vm_bin_mad, vm_bin_med+vm_bin_mad, color='g', alpha=0.1)\n",
    "#         #ax4.set_xlim(0, 50)\n",
    "#         axa[0,nplot].xaxis.tick_bottom()\n",
    "#         axa[0,nplot].xaxis.set_label_position(\"bottom\")\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "        \n",
    "#         nplot += 1\n",
    "# #         axa[0,nplot].set_xlabel('divQ (??)', fontsize=fs)\n",
    "# #         axa[0,nplot].plot(vm_bin_med, z_bin_centers, color='g', label='Vm (%0.2f m/yr)' % gf.vm_mean)\n",
    "# #         axa[0,nplot].fill_betweenx(z_bin_centers, vm_bin_med-vm_bin_mad, vm_bin_med+vm_bin_mad, color='g', alpha=0.1)\n",
    "# #         #ax4.set_xlim(0, 50)\n",
    "# #         axa[0,nplot].xaxis.tick_bottom()\n",
    "# #         axa[0,nplot].xaxis.set_label_position(\"bottom\")\n",
    "# #         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "# #         gf.divQ\n",
    "    \n",
    "# #     if gf.vm is not None:\n",
    "# #         nplot += 1\n",
    "# # #         ax4 = axa[0,nplot].twinx()\n",
    "# #         axa[0,nplot].set_xlabel('Velocity (m/yr)', fontsize=fs)\n",
    "# #         axa[0,nplot].plot(vm_bin_med, z_bin_centers, color='g', label='Vm (%0.2f m/yr)' % gf.vm_mean)\n",
    "# #         axa[0,nplot].fill_betweenx(z_bin_centers, vm_bin_med-vm_bin_mad, vm_bin_med+vm_bin_mad, color='g', alpha=0.1)\n",
    "# #         #ax4.set_xlim(0, 50)\n",
    "# #         axa[0,nplot].xaxis.tick_bottom()\n",
    "# #         axa[0,nplot].xaxis.set_label_position(\"bottom\")\n",
    "# #         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     #Make room for suptitle\n",
    "#     plt.subplots_adjust(top=0.95, wspace=0.1)\n",
    "#     #print(\"Saving aed plot\")\n",
    "#     fig_fn = os.path.join(outdir_fig, gf.feat_fn+'_mb_aed.png')\n",
    "#     #plt.savefig(fig_fn, bbox_inches='tight', dpi=300)\n",
    "#     plt.savefig(fig_fn, dpi=300)\n",
    "#     plt.close(f)\n",
    "    \n",
    "    \n",
    "    outbins_df = pd.DataFrame(outbins, columns=outbins_header.split(','))\n",
    "    return outbins_df, z_bin_edges\n",
    "#     return z_bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\"\"\"\n",
    "Compute debris thickness through sub-debris and temperature inversion methods\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy import ndimage\n",
    "import xarray as xr\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "from pygeotools.lib import malib, warplib, geolib, iolib, timelib\n",
    "# from imview.lib import pltlib\n",
    "\n",
    "import globaldebris_input as input\n",
    "\n",
    "verbose=False\n",
    "extra_layers=True\n",
    "min_glac_area_writeout = input.min_glac_area_writeout\n",
    "min_valid_area_perc = input.min_valid_area_perc\n",
    "buff_dist = input.buff_dist\n",
    "# bin_width = input.emvel_bin_width\n",
    "#Surface to column average velocity scaling\n",
    "v_col_f = input.v_col_f\n",
    "csv_ending = input.output_emvel_csv_ending\n",
    "emvel_filter_pixsize = input.emvel_filter_pixsize\n",
    "\n",
    "#INPUT\n",
    "glac_shp_fn_dict = {'13':input.main_directory + '/../../../HiMAT/RGI/rgi60/13_rgi60_CentralAsia/13_rgi60_CentralAsia.shp',\n",
    "                    '14':input.main_directory + '/../../../HiMAT/RGI/rgi60/14_rgi60_SouthAsiaWest/14_rgi60_SouthAsiaWest.shp',\n",
    "                    '15':input.main_directory + '/../../../HiMAT/RGI/rgi60/15_rgi60_SouthAsiaEast/15_rgi60_SouthAsiaEast.shp'}\n",
    "glac_shp_proj_fp = input.output_fp + 'glac_shp_proj/'\n",
    "if os.path.exists(glac_shp_proj_fp) == False:\n",
    "    os.makedirs(glac_shp_proj_fp)\n",
    "\n",
    "\n",
    "#DEM\n",
    "z1_dir_sample = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/surface_DEMs_RGI60/' + \n",
    "          'surface_DEMs_RGI60-XXXX/')\n",
    "z1_fn_sample = 'surface_DEM_RGI60-XXXX.tif'\n",
    "# Ice thickness\n",
    "huss_dir_sample = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/' + \n",
    "                   'composite_thickness_RGI60-all_regions/RGI60-XXXX/')\n",
    "huss_fn_sample = 'RGI60-XXXX_thickness.tif'\n",
    "#Surface velocity\n",
    "v_dir = input.main_directory + '/../../../Satellite_Images/ITS_Live'\n",
    "vx_fn = os.path.join(v_dir, 'HMA_G0120_0000_vx.tif')\n",
    "vy_fn = os.path.join(v_dir, 'HMA_G0120_0000_vy.tif')\n",
    "\n",
    "# topdir='/Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/'\n",
    "# #Output directory\n",
    "# outdir = topdir + 'Shean_2019_0213/mb_combined_20190213_nmad_bins/'\n",
    "# outdir_fig = outdir + '/figures/'\n",
    "outdir_csv = input.outdir_emvel_fp \n",
    "outdir_fig = input.outdir_emvel_fp  + '../figures/'\n",
    "\n",
    "if os.path.exists(glac_shp_proj_fp) == False:\n",
    "    os.makedirs(glac_shp_proj_fp)\n",
    "if os.path.exists(outdir_csv) == False:\n",
    "    os.makedirs(outdir_csv)\n",
    "if os.path.exists(outdir_fig) == False:\n",
    "    os.makedirs(outdir_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8053\n",
      "4182 glaciers in region 13 are included in this model run: ['00062', '00093', '00130', '00135', '00137', '00140', '00147', '00175', '00181', '00183', '00203', '00210', '00277', '00358', '00382', '00386', '00391', '00394', '00400', '00401', '00403', '00439', '00440', '00441', '00465', '00561', '00585', '00594', '00604', '00606', '00611', '00628', '00643', '00693', '00713', '00750', '00751', '00757', '00761', '00763', '00777', '00788', '00809', '00830', '00834', '00838', '00880', '00884', '00885', '00891'] and more\n",
      "2468 glaciers in region 14 are included in this model run: ['00005', '00018', '00032', '00036', '00043', '00057', '00063', '00072', '00088', '00101', '00104', '00111', '00131', '00142', '00145', '00146', '00159', '00163', '00164', '00187', '00213', '00219', '00222', '00225', '00235', '00243', '00251', '00271', '00287', '00309', '00323', '00326', '00336', '00346', '00347', '00352', '00353', '00363', '00366', '00367', '00370', '00372', '00380', '00398', '00403', '00432', '00449', '00453', '00456', '00466'] and more\n",
      "1403 glaciers in region 15 are included in this model run: ['00024', '00026', '00055', '00057', '00107', '00186', '00194', '00232', '00233', '00234', '00288', '00355', '00356', '00358', '00368', '00372', '00379', '00399', '00406', '00410', '00423', '00475', '00503', '00612', '00617', '00621', '00639', '00642', '00643', '00647', '00648', '00655', '00679', '00726', '00835', '00850', '00855', '00868', '00869', '00872', '00880', '00881', '00885', '00894', '00898', '00899', '00909', '00910', '00911', '00920'] and more\n",
      "This study is focusing on 8053 glaciers in region [13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "rgiid_list = []\n",
    "rgiid_fn_list = []\n",
    "for i in os.listdir(input.mb_binned_fp):\n",
    "    if i.endswith('mb_bins.csv'):\n",
    "        rgiid_list.append(i[0:8])\n",
    "        rgiid_fn_list.append(i)\n",
    "        \n",
    "rgiid_list = sorted(rgiid_list)\n",
    "rgiid_fn_list = sorted(rgiid_fn_list)\n",
    "\n",
    "print(len(rgiid_list))\n",
    "\n",
    "main_glac_rgi = selectglaciersrgitable(rgiid_list)\n",
    "main_glac_rgi['bin_fn'] = rgiid_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2896 RGI60-13.40068\n",
      "\n",
      "\n",
      "HACK TO BYPASS VALID AREA\n",
      "\n",
      "\n",
      "13.40068 has no glacier area!\n"
     ]
    }
   ],
   "source": [
    "# for nglac, glac_idx in enumerate(main_glac_rgi.index.values):\n",
    "# for nglac, glac_idx in enumerate([main_glac_rgi.index.values[6940]]):\n",
    "for nglac, glac_idx in enumerate([main_glac_rgi.index.values[2896]]):\n",
    "    glac_str = main_glac_rgi.loc[glac_idx,'rgino_str']\n",
    "    rgiid = main_glac_rgi.loc[glac_idx,'RGIId']\n",
    "    \n",
    "    print(nglac, glac_idx, rgiid)\n",
    "    \n",
    "    out_csv_fn = os.path.join(outdir_csv, glac_str + csv_ending)\n",
    "    if verbose:\n",
    "        print('output_fn:', out_csv_fn)\n",
    "\n",
    "    if not os.path.exists(out_csv_fn):\n",
    "    \n",
    "        region = glac_str.split('.')[0]\n",
    "\n",
    "        # Shape layer processing\n",
    "        glac_shp_init = gpd.read_file(glac_shp_fn_dict[region])\n",
    "        if verbose:\n",
    "            print('Shp init crs:', glac_shp_init.crs)\n",
    "\n",
    "        glac_shp_single = glac_shp_init[glac_shp_init['RGIId'] == rgiid]\n",
    "        glac_shp_single = glac_shp_single.reset_index()\n",
    "\n",
    "        # Project shapefile\n",
    "        huss_dir = huss_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "        huss_fn = huss_fn_sample.replace('XXXX',glac_str)\n",
    "\n",
    "        proj_fn = os.path.join(huss_dir, huss_fn) # THIS PROJECTION IS KEY!\n",
    "        ds = gdal.Open(proj_fn)\n",
    "        prj = ds.GetProjection()\n",
    "        srs = osr.SpatialReference(wkt=prj)\n",
    "        aea_srs = srs\n",
    "\n",
    "        # If projected shapefile already exists, then skip projection\n",
    "        glac_shp_proj_fn = glac_shp_proj_fp + glac_str + '_crs' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp'\n",
    "\n",
    "        if os.path.exists(glac_shp_proj_fn) == False:\n",
    "            glac_shp_proj = glac_shp_single.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "            glac_shp_proj.to_file(glac_shp_proj_fn)\n",
    "\n",
    "        glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "        glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "        #This should be contained in features\n",
    "        glac_shp_srs = glac_shp_lyr.GetSpatialRef()\n",
    "        feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "        if verbose:\n",
    "            print(\"Input glacier polygon count: %i\" % feat_count)\n",
    "\n",
    "        # Load DEM\n",
    "        z1_dir = z1_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "        z1_fn = z1_fn_sample.replace('XXXX',glac_str)\n",
    "        z1_ds = gdal.Open(z1_dir + z1_fn)\n",
    "        z1_int_geom = geolib.ds_geom_intersection([z1_ds, z1_ds], t_srs=glac_shp_srs)\n",
    "\n",
    "        glacfeat_list = []\n",
    "        glacname_fieldname = \"Name\"\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        glacnum_fieldname = \"RGIId\"\n",
    "        glacnum_fmt = '%08.5f'\n",
    "\n",
    "        for n, feat in enumerate(glac_shp_lyr):\n",
    "            gf = GlacFeat(feat, glacname_fieldname, glacnum_fieldname)\n",
    "            if verbose:\n",
    "                print(\"%i of %i: %s\" % (n+1, feat_count, gf.feat_fn))\n",
    "            #NOTE: Input must be in projected coordinate system, ideally equal area\n",
    "            #Should check this and reproject\n",
    "            gf.geom_attributes(srs=aea_srs)\n",
    "            glacfeat_list.append(gf)\n",
    "\n",
    "        if verbose:\n",
    "            print(gf.feat_fn)\n",
    "        \n",
    "        fn_dict = OrderedDict()\n",
    "        #We at least want to warp the two input DEMs\n",
    "        fn_dict['z1'] = os.path.join(z1_dir, z1_fn)\n",
    "\n",
    "        if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "            if verbose:\n",
    "                print(gf.glacnum)\n",
    "\n",
    "            # Ice thickness data\n",
    "            ice_thick_fn = os.path.join(huss_dir, huss_fn)\n",
    "            if os.path.exists(ice_thick_fn):\n",
    "                fn_dict['ice_thick'] = ice_thick_fn\n",
    "\n",
    "            if verbose:\n",
    "                print(fn_dict['ice_thick'])\n",
    "\n",
    "            if os.path.exists(vx_fn):\n",
    "                fn_dict['vx'] = vx_fn\n",
    "                fn_dict['vy'] = vy_fn\n",
    "\n",
    "    #         if os.path.exists(ts_fullfn):\n",
    "    #             fn_dict['ts'] = ts_fullfn\n",
    "\n",
    "    #         if os.path.exists(debris_fullfn):\n",
    "    #             fn_dict['debris_thick_ts'] = debris_fullfn\n",
    "\n",
    "        #Expand extent to include buffered region around glacier polygon\n",
    "        warp_extent = geolib.pad_extent(gf.glac_geom_extent, width=buff_dist)\n",
    "        if verbose:\n",
    "            print(\"Expanding extent\")\n",
    "            print(gf.glac_geom_extent)\n",
    "            print(warp_extent)\n",
    "            print(aea_srs)\n",
    "\n",
    "        #Warp everything to common res/extent/proj\n",
    "        ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res='min', \\\n",
    "                extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "                r='cubic')\n",
    "\n",
    "        ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "\n",
    "        if verbose:\n",
    "            print(ds_list)\n",
    "            print(fn_dict.keys())\n",
    "\n",
    "        #Prepare mask for all glaciers within buffered area, not just the current glacier polygon\n",
    "        glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "        glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "\n",
    "        #Get global glacier mask\n",
    "        #Want this to be True over ALL glacier surfaces, not just the current polygon\n",
    "        glac_shp_lyr_mask = geolib.lyr2mask(glac_shp_lyr, ds_dict['ice_thick'])\n",
    "\n",
    "        #Create buffer around glacier polygon\n",
    "        glac_geom_buff = gf.glac_geom.Buffer(buff_dist)\n",
    "        #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "        glac_geom_buff_mask = geolib.geom2mask(glac_geom_buff, ds_dict['ice_thick'])\n",
    "\n",
    "        # ds masks\n",
    "        ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "        dem1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "        dems_mask = dem1.mask\n",
    "        if verbose:\n",
    "            print('list of datasets:', len(ds_list_masked), fn_dict.values())\n",
    "\n",
    "        #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "        static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "        static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "\n",
    "        if 'z1' in ds_dict:\n",
    "            #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "            glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "            gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']))\n",
    "            #gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "\n",
    "            if verbose:\n",
    "                print('\\n\\n# z1 pixels:', gf.z1.count(), '\\n')\n",
    "            if gf.z1.count() == 0:\n",
    "                if verbose:\n",
    "                    print(\"No z1 pixels\")\n",
    "        else:\n",
    "            print(\"Unable to load z1 ds\")\n",
    "        \n",
    "        if nglac == 0:\n",
    "            print('\\n\\nHACK TO BYPASS VALID AREA\\n\\n')\n",
    "        gf.valid_area_perc = 100\n",
    "\n",
    "        if gf.valid_area_perc < (100. * min_valid_area_perc):\n",
    "            if verbose:\n",
    "                print(\"Not enough valid pixels. %0.1f%% percent of glacier polygon area\" % (gf.valid_area_perc))\n",
    "        #     return None\n",
    "\n",
    "        else:\n",
    "            #Filter dz - throw out abs differences >150 m\n",
    "\n",
    "            #Compute dz, volume change, mass balance and stats\n",
    "            gf.z1_stats = malib.get_stats(gf.z1)\n",
    "            z1_elev_med = gf.z1_stats[5]\n",
    "            z1_elev_min, z1_elev_max = malib.calcperc(gf.z1, (0.1, 99.9))\n",
    "\n",
    "            #Caluclate stats for aspect and slope using z2\n",
    "            #Requires GDAL 2.1+\n",
    "            gf.z1_aspect = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='aspect', returnma=True), mask=glac_geom_mask)\n",
    "            gf.z1_aspect_stats = malib.get_stats(gf.z1_aspect)\n",
    "            z1_aspect_med = gf.z1_aspect_stats[5]\n",
    "            gf.z1_slope = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='slope', returnma=True), mask=glac_geom_mask)\n",
    "            gf.z1_slope_stats = malib.get_stats(gf.z1_slope)\n",
    "            z1_slope_med = gf.z1_slope_stats[5]\n",
    "\n",
    "            #Can estimate ELA values computed from hypsometry and typical AAR\n",
    "            #For now, assume ELA is mean\n",
    "            gf.z1_ela = None\n",
    "            gf.z1_ela = gf.z1_stats[3]\n",
    "            #Note: in theory, the ELA should get higher with mass loss\n",
    "            #In practice, using mean and same polygon, ELA gets lower as glacier surface thins\n",
    "\n",
    "\n",
    "            if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "                if 'ice_thick' in ds_dict:\n",
    "                    #Load ice thickness\n",
    "                    gf.H = np.ma.array(iolib.ds_getma(ds_dict['ice_thick']), mask=glac_geom_mask)\n",
    "                    gf.H_mean = gf.H.mean()\n",
    "                    if verbose:\n",
    "                        print('mean ice thickness [m]:', gf.H_mean)\n",
    "\n",
    "                if 'vx' in ds_dict and 'vy' in ds_dict:\n",
    "                    #Load surface velocity maps\n",
    "                    gf.vx = np.ma.array(iolib.ds_getma(ds_dict['vx']), mask=glac_geom_mask)\n",
    "                    gf.vy = np.ma.array(iolib.ds_getma(ds_dict['vy']), mask=glac_geom_mask)\n",
    "                    gf.vm = np.ma.sqrt(gf.vx**2 + gf.vy**2)\n",
    "                    gf.vm_mean = gf.vm.mean()\n",
    "                    if verbose:\n",
    "                        print('mean velocity [m/s]:', gf.vm_mean)\n",
    "\n",
    "                    if gf.H is not None:\n",
    "                        #Compute flux\n",
    "                        gf.Q = gf.H * v_col_f * np.array([gf.vx, gf.vy])\n",
    "                        #Note: np.gradient returns derivatives relative to axis number, so (y, x) in this case\n",
    "                        #Want x-derivative of x component\n",
    "                        gf.divQ = np.gradient(gf.Q[0])[1] + np.gradient(gf.Q[1])[0]\n",
    "\n",
    "        #                 gf.divQ = gf.H*(np.gradient(v_col_f*gf.vx)[1] + np.gradient(v_col_f*gf.vy)[0]) \\\n",
    "        #                         + v_col_f*gf.vx*(np.gradient(gf.H)[1]) + v_col_f*gf.vy*(np.gradient(gf.H)[0])\n",
    "\n",
    "                        #Should smooth divQ, better handling of data gaps\n",
    "\n",
    "                if 'ts' in ds_dict:\n",
    "                    #Load surface temperature maps\n",
    "                    gf.ts = np.ma.array(iolib.ds_getma(ds_dict['ts']), mask=glac_geom_mask)\n",
    "                else:\n",
    "                    gf.ts = None\n",
    "\n",
    "                if 'debris_thick_ts' in ds_dict:\n",
    "                    # Load debris thickness map\n",
    "                    gf.debris_thick_ts = np.ma.array(iolib.ds_getma(ds_dict['debris_thick_ts']), mask=glac_geom_mask)\n",
    "                    gf.meltfactor_ts = None\n",
    "                else:\n",
    "                    gf.debris_thick_ts = None\n",
    "                    gf.meltfactor_ts = None\n",
    "\n",
    "            if verbose:\n",
    "                print('Area [km2]:', gf.glac_area / 1e6)\n",
    "                print('-------------------------------')\n",
    "\n",
    "\n",
    "            # Plots\n",
    "    #         titles = ['Z1']\n",
    "    #         z1_full2plot = gf.z1\n",
    "    #         z1_full2plot.mask = dems_mask\n",
    "    #         clim = malib.calcperc(z1_full2plot, (2,98))\n",
    "    #         plot_array(z1_full2plot, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=outdir_fig + glac_str + '_dem.png')\n",
    "\n",
    "            #Now apply glacier mask AND mask NaN values\n",
    "            glac_geom_mask = np.ma.mask_or(glac_geom_mask, dems_mask)\n",
    "            # nan_mask = np.ma.masked_invalid(gf.dz)\n",
    "            # glac_geom_mask = np.ma.mask_or(glac_geom_mask, nan_mask.mask)\n",
    "            gf.z1 = np.ma.array(gf.z1, mask=glac_geom_mask)\n",
    "\n",
    "            gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "            titles = ['Z1 (masked)']\n",
    "            clim = malib.calcperc(gf.z1, (2,98))\n",
    "            plot_array(gf.z1, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=outdir_fig + glac_str + '_dem.png')\n",
    "\n",
    "            if verbose:\n",
    "                print(gf.z1.shape)\n",
    "                \n",
    "#             titles = ['Vx']\n",
    "#             var_full2plot = gf.vx\n",
    "#             var_full2plot.mask = glac_geom_mask\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, titles, 'inferno', 'vx', fn=outdir_fig + gf.feat_fn +'_vx.png')\n",
    "\n",
    "#             titles = ['Vy']\n",
    "#             var_full2plot = gf.vy\n",
    "#             var_full2plot.mask = glac_geom_mask\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, titles, 'inferno', 'vy', fn=outdir_fig + gf.feat_fn +'_vy.png')\n",
    "\n",
    "            gf.vtot = (gf.vx**2 + gf.vy**2)**0.5\n",
    "\n",
    "            titles = ['Velocity (m/yr)']\n",
    "            var_full2plot = gf.vtot\n",
    "            var_full2plot.mask = glac_geom_mask\n",
    "            clim = malib.calcperc(var_full2plot, (2,98))\n",
    "            plot_array(var_full2plot, clim, titles, 'inferno', 'Velocity (m/yr)', fn=outdir_fig + glac_str +'_velocity.png')\n",
    "\n",
    "            titles = ['Ice thickness']\n",
    "            var_full2plot = gf.H\n",
    "            var_full2plot.mask = glac_geom_mask\n",
    "            clim = malib.calcperc(var_full2plot, (2,98))\n",
    "            plot_array(var_full2plot, clim, titles, 'inferno', 'H', fn=outdir_fig + gf.feat_fn +'_ice_thickness.png')\n",
    "            \n",
    "#             titles = ['Flux']\n",
    "#             divQ_full2plot = gf.divQ\n",
    "#             divQ_full2plot.mask = glac_geom_mask\n",
    "#             clim = malib.calcperc(divQ_full2plot, (2,98))\n",
    "#             plot_array(divQ_full2plot, clim, titles, 'inferno', 'divQ', fn=outdir_fig + glac_str +'_divQ.png')\n",
    "\n",
    "            # ===== \"COREGISTER\" SURFACE LOWERING WITH DEM USED FOR ICE THICKNESS =====\n",
    "            # Load Mass Balance Data and find displacement =====\n",
    "            if verbose:\n",
    "                print('\\nREALLY THIS SHOULD BE DONE BY COREGISTRATION OF THE TWO DEMS\\n')\n",
    "            mb_df_fn = main_glac_rgi.loc[glac_idx,'bin_fn']\n",
    "            mb_df = pd.read_csv(input.mb_binned_fp + mb_df_fn)\n",
    "            mb_df.loc[:,:] = mb_df.values.astype(np.float64)\n",
    "            mb_bin0_km2 = mb_df.loc[0,' z1_bin_area_perc'] / 100 * main_glac_rgi.loc[glac_idx,'Area']\n",
    "            mb_bin_size = mb_df.loc[1,'# bin_center_elev_m'] - mb_df.loc[0,'# bin_center_elev_m']\n",
    "            pix_km2 = gf.res[0] * gf.res[1] / (1000)**2\n",
    "            if verbose:\n",
    "                print('total glacier area [km2]:', main_glac_rgi.loc[glac_idx,'Area'])\n",
    "                print('initial bin area [km2]:', mb_bin0_km2)\n",
    "                print('bin size [m]:', mb_bin_size)\n",
    "                print('pixel size [km2]:', pix_km2)\n",
    "            # Find displacement\n",
    "            if len(gf.z1.compressed()) > 0:\n",
    "                z1 = gf.z1.compressed()\n",
    "                z1_min = z1[z1>0].min()\n",
    "                z1_max = z1[z1>0].max()\n",
    "                z1_km2 = 0\n",
    "                elev = int(z1_min)\n",
    "                while z1_km2 < mb_bin0_km2 and elev < z1_max:\n",
    "                    elev += 1\n",
    "                    z1_idx = np.where((z1 > 0) & (z1 < elev))\n",
    "                    if len(z1_idx[0]) > 0:\n",
    "                        z1_km2 = len(z1_idx[0]) * pix_km2\n",
    "                #         print(elev, z1_km2)\n",
    "                if verbose:\n",
    "                    print(elev, z1_km2, 'vs', mb_df.loc[0,'# bin_center_elev_m'], mb_bin0_km2)\n",
    "                mb_bin0_upper =  mb_df.loc[0,'# bin_center_elev_m'] + mb_bin_size / 2\n",
    "                z1_offset = elev - mb_bin0_upper\n",
    "                if verbose:\n",
    "                    print('z1_offset:', z1_offset)\n",
    "                # Update z1 with the offset\n",
    "                mask_offset = np.ma.array(np.zeros(gf.z1.mask.shape) - z1_offset, mask=np.ma.getmask(gf.z1))\n",
    "                gf.z1[~gf.z1.mask] = gf.z1[~gf.z1.mask] + mask_offset[~mask_offset.mask]\n",
    "\n",
    "                # ===== EMERGENCE VELOCITY =====\n",
    "                vx = np.ma.filled(gf.vx,0)\n",
    "                vy = np.ma.filled(gf.vy,0)\n",
    "                H = np.ma.filled(gf.H,0)\n",
    "                vx[gf.z1 > gf.z1.max()] = 0\n",
    "                vy[gf.z1 > gf.z1.max()] = 0\n",
    "                H[gf.z1 > gf.z1.max()] = 0\n",
    "                vmax = np.nanmax((vx**2 + vy**2)**0.5)\n",
    "\n",
    "                # Emergence computation\n",
    "                emvel = emergence_pixels(gf, vx, vy, H, gf.res[0], gf.res[1], \n",
    "                                         positive_is_east=True, positive_is_north=True, \n",
    "                                         constant_icethickness=False, max_velocity=vmax, vel_min=0, debug=False)\n",
    "                # 3x3 filter to reduce\n",
    "                if emvel_filter_pixsize > 0:\n",
    "                    emvel = ndimage.filters.convolve(emvel, weights=np.full((emvel_filter_pixsize, emvel_filter_pixsize), \n",
    "                                                                            1.0/emvel_filter_pixsize**2))\n",
    "                # Add to glacier feature\n",
    "                gf.emvel = np.ma.masked_array(emvel, mask=np.ma.getmask(gf.z1))\n",
    "\n",
    "                # ===== EXPORT BINNED STATISTICS =====\n",
    "                #Do AED for all\n",
    "                #Compute mb using scaled AED vs. polygon\n",
    "                #Check for valid pixel count vs. feature area, fill if appropriate\n",
    "                if gf.glac_area_km2 > min_glac_area_writeout:\n",
    "                    outbins_df, z_bin_edges = hist_plot(gf, bin_width=mb_bin_size, csv_ending=csv_ending)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(outbins_df.loc[0:10,['bin_center_elev_m', ' vm_med',' vm_mad', ' H_mean', ' H_std', \n",
    "                                                   ' emvel_mean', ' emvel_std',' emvel_med', ' emvel_mad']])\n",
    "            else:\n",
    "                print('\\n' + glac_str + ' HAS NO GLACIER AREA!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
